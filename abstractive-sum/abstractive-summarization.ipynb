{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extractive_summarization_modified.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OBZHxEHjxmKl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b877cfbc-8a14-40f9-86fb-58eb619d3e75"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "4tO2YZdFkFsm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "8d0e6e0d-e7a1-4987-ad1c-f6eba7e205e0"
      },
      "cell_type": "code",
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.11.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GPIF7KozgYen",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a12073f4-2381-4b0c-d377-b0d9b5ad3376"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import tensor_array_ops\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4Ng5VGqSlWqy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ixur21pIq2Rp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "download = drive.CreateFile({'id': '1K0-uCIFQoFRQF3btG0GTfh8WGcEUbJu_'})\n",
        "download.GetContentFile('numberbatch-en-17.02.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CoJOyWRarvNb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "download = drive.CreateFile({'id': '1s_MNlv4kvQ2dQKaYbSccyvTXtD5s89Mp'})\n",
        "download.GetContentFile('Reviews.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AisyiJhxgYeu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def __pickleStuff(filename, stuff):\n",
        "    save_stuff = open(filename, \"wb\")\n",
        "    pickle.dump(stuff, save_stuff)\n",
        "    save_stuff.close()\n",
        "def __loadStuff(filename):\n",
        "    saved_stuff = open(filename,\"rb\")\n",
        "    stuff = pickle.load(saved_stuff)\n",
        "    saved_stuff.close()\n",
        "    return stuff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9PXo6MA1gYex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "dd3f2957-05cb-4468-92ba-d4c0f888e882"
      },
      "cell_type": "code",
      "source": [
        "#Reviews.csv contains amazon fine foods reviews\n",
        "reviews = pd.read_csv(\"Reviews.csv\")\n",
        "# Remove null values and unneeded features\n",
        "reviews = reviews.dropna()\n",
        "reviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator',\n",
        "                        'Score','Time'], 1)\n",
        "reviews = reviews.reset_index(drop=True)\n",
        "reviews.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Summary                                               Text\n",
              "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
              "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
              "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
              "4            Great taffy  Great taffy at a great price.  There was a wid..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "9Q4kCH-XgYe0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SryayARWgYe2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_text(text, remove_stopwords = True):\n",
        "    '''Remove unwanted characters, and format the text to create fewer nulls word embeddings'''\n",
        "    \n",
        "    # Convert words to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Replace contractions with their longer forms \n",
        "    if True:\n",
        "        # We are not using \"text.split()\" here\n",
        "        #since it is not fool proof, e.g. words followed by punctuations \"Are you kidding?I think you aren't.\"\n",
        "        text = re.findall(r\"[\\w']+\", text)\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "    \n",
        "    # Format words and remove unwanted characters\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)# remove links\n",
        "    text = re.sub(r'\\<a href', ' ', text)# remove html link tag\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rQTnjSwOgYe4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fcee6218-2010-4e0f-f55d-6b1c62b15a4d"
      },
      "cell_type": "code",
      "source": [
        "clean_summaries = []\n",
        "for summary in reviews.Summary:\n",
        "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
        "print(\"Summaries are complete.\")\n",
        "\n",
        "clean_texts = []\n",
        "for text in reviews.Text:\n",
        "    clean_texts.append(clean_text(text))\n",
        "print(\"Texts are complete.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summaries are complete.\n",
            "Texts are complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nRUlVOSfgYe9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b8ce64a-9bed-4b85-b1d7-0329291720b0"
      },
      "cell_type": "code",
      "source": [
        "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
        "embeddings_index = {}\n",
        "with open('numberbatch-en-17.02.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings:', len(embeddings_index))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word embeddings: 484557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ptE-oHcbgYfA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1fc391b8-26b3-4921-abbe-6a7c39eedf73"
      },
      "cell_type": "code",
      "source": [
        "def count_words(count_dict, text):\n",
        "    '''Count the number of occurrences of each word in a set of text'''\n",
        "    for sentence in text:\n",
        "        for word in sentence.split():\n",
        "            if word not in count_dict:\n",
        "                count_dict[word] = 1\n",
        "            else:\n",
        "                count_dict[word] += 1\n",
        "                \n",
        "word_counts = {}\n",
        "\n",
        "count_words(word_counts, clean_summaries)\n",
        "count_words(word_counts, clean_texts)\n",
        "            \n",
        "print(\"Size of Vocabulary:\", len(word_counts))\n",
        "\n",
        "missing_words = 0\n",
        "threshold = 20\n",
        "\n",
        "for word, count in word_counts.items():\n",
        "    if count > threshold:\n",
        "        if word not in embeddings_index:\n",
        "            missing_words += 1\n",
        "            \n",
        "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
        "            \n",
        "print(\"Number of words missing from CN:\", missing_words)\n",
        "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Vocabulary: 125885\n",
            "Number of words missing from CN: 1757\n",
            "Percent of words that are missing from vocabulary: 1.4000000000000001%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4IyxJfKdgYfE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "69658411-87a3-4e1a-d5d3-c6a68b39a2fa"
      },
      "cell_type": "code",
      "source": [
        "#dictionary to convert words to integers\n",
        "vocab_to_int = {} \n",
        "\n",
        "value = 0\n",
        "for word, count in word_counts.items():\n",
        "    if count >= threshold or word in embeddings_index:\n",
        "        vocab_to_int[word] = value\n",
        "        value += 1\n",
        "\n",
        "# Special tokens that will be added to our vocab\n",
        "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
        "\n",
        "# Add codes to vocab\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = len(vocab_to_int)\n",
        "\n",
        "# Dictionary to convert integers to words\n",
        "int_to_vocab = {}\n",
        "for word, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = word\n",
        "\n",
        "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
        "\n",
        "print(\"Total number of unique words:\", len(word_counts))\n",
        "print(\"Number of words we will use:\", len(vocab_to_int))\n",
        "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of unique words: 125885\n",
            "Number of words we will use: 65163\n",
            "Percent of words we will use: 51.76%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GYtwQ4vcgYfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21e8e2a0-90d4-4365-fbb5-5a4cf261e30f"
      },
      "cell_type": "code",
      "source": [
        "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
        "embedding_dim = 300\n",
        "nb_words = len(vocab_to_int)\n",
        "\n",
        "# Create matrix with default values of zero\n",
        "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
        "for word, i in vocab_to_int.items():\n",
        "    if word in embeddings_index:\n",
        "        word_embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "        # If word not in CN, create a random embedding for it\n",
        "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "        embeddings_index[word] = new_embedding\n",
        "        word_embedding_matrix[i] = new_embedding\n",
        "\n",
        "# Check if value matches len(vocab_to_int)\n",
        "print(len(word_embedding_matrix))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wYZkB9MvgYfJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d2408f84-e1e1-465e-d0f5-73b819f2fe58"
      },
      "cell_type": "code",
      "source": [
        "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
        "    '''Convert words in text to an integer.\n",
        "       If word is not in vocab_to_int, use UNK's integer.\n",
        "       Total the number of words and UNKs.\n",
        "       Add EOS token to the end of texts'''\n",
        "    ints = []\n",
        "    for sentence in text:\n",
        "        sentence_ints = []\n",
        "        for word in sentence.split():\n",
        "            word_count += 1\n",
        "            if word in vocab_to_int:\n",
        "                sentence_ints.append(vocab_to_int[word])\n",
        "            else:\n",
        "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
        "                unk_count += 1\n",
        "        if eos:\n",
        "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
        "        ints.append(sentence_ints)\n",
        "    return ints, word_count, unk_count\n",
        "\n",
        "# Apply convert_to_ints to clean_summaries and clean_texts\n",
        "word_count = 0\n",
        "unk_count = 0\n",
        "\n",
        "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
        "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
        "\n",
        "unk_percent = round(unk_count/word_count,4)*100\n",
        "\n",
        "print(\"Total number of words in headlines:\", word_count)\n",
        "print(\"Total number of UNKs in headlines:\", unk_count)\n",
        "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words in headlines: 49652961\n",
            "Total number of UNKs in headlines: 141058\n",
            "Percent of words that are UNK: 0.27999999999999997%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M_dN8SpWgYfL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "6178018b-5668-43f4-fb54-d974a3b1b718"
      },
      "cell_type": "code",
      "source": [
        "def create_lengths(text):\n",
        "    '''Create a data frame of the sentence lengths from a text'''\n",
        "    lengths = []\n",
        "    for sentence in text:\n",
        "        lengths.append(len(sentence))\n",
        "    return pd.DataFrame(lengths, columns=['counts'])\n",
        "\n",
        "lengths_summaries = create_lengths(int_summaries)\n",
        "lengths_texts = create_lengths(int_texts)\n",
        "\n",
        "print(\"Summaries:\")\n",
        "print(lengths_summaries.describe())\n",
        "print()\n",
        "print(\"Texts:\")\n",
        "print(lengths_texts.describe())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summaries:\n",
            "              counts\n",
            "count  568411.000000\n",
            "mean        4.181212\n",
            "std         2.657213\n",
            "min         0.000000\n",
            "25%         2.000000\n",
            "50%         4.000000\n",
            "75%         5.000000\n",
            "max        48.000000\n",
            "\n",
            "Texts:\n",
            "              counts\n",
            "count  568411.000000\n",
            "mean       84.172764\n",
            "std        83.095213\n",
            "min         4.000000\n",
            "25%        35.000000\n",
            "50%        59.000000\n",
            "75%       102.000000\n",
            "max      3530.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yWcsyyiJgYfN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def unk_counter(sentence):\n",
        "    '''Counts the number of time UNK appears in a sentence.'''\n",
        "    unk_count = 0\n",
        "    for word in sentence:\n",
        "        if word == vocab_to_int[\"<UNK>\"]:\n",
        "            unk_count += 1\n",
        "    return unk_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "73hYEF3ygYfP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aab19aa5-24f2-414f-fe04-dd1e69551f5b"
      },
      "cell_type": "code",
      "source": [
        "max_text_length = 83 # This will cover up to 89.5% lengthes\n",
        "max_summary_length = 13 # This will cover up to 99% lengthes\n",
        "min_length = 2\n",
        "unk_text_limit = 1 # text can contain up to 1 UNK word\n",
        "unk_summary_limit = 0 # Summary should not contain any UNK word\n",
        "\n",
        "def filter_condition(item):\n",
        "    int_summary = item[0]\n",
        "    int_text = item[1]\n",
        "    if(len(int_summary) >= min_length and \n",
        "       len(int_summary) <= max_summary_length and \n",
        "       len(int_text) >= min_length and \n",
        "       len(int_text) <= max_text_length and \n",
        "       unk_counter(int_summary) <= unk_summary_limit and \n",
        "       unk_counter(int_text) <= unk_text_limit):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "int_text_summaries = list(zip(int_summaries , int_texts))\n",
        "int_text_summaries_filtered = list(filter(filter_condition, int_text_summaries))\n",
        "sorted_int_text_summaries = sorted(int_text_summaries_filtered, key=lambda item: len(item[1]))\n",
        "sorted_int_text_summaries = list(zip(*sorted_int_text_summaries))\n",
        "sorted_summaries = list(sorted_int_text_summaries[0])\n",
        "sorted_texts = list(sorted_int_text_summaries[1])\n",
        "# Delete those temporary varaibles\n",
        "del int_text_summaries, sorted_int_text_summaries, int_text_summaries_filtered\n",
        "# Compare lengths to ensure they match\n",
        "print(len(sorted_summaries))\n",
        "print(len(sorted_texts))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "318677\n",
            "318677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kAW3KxzngYfS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "0c31b22c-ccaf-496d-af3d-a4f539ccc1b4"
      },
      "cell_type": "code",
      "source": [
        "__pickleStuff(\"./data/clean_summaries.p\",clean_summaries)\n",
        "__pickleStuff(\"./data/clean_texts.p\",clean_texts)\n",
        "\n",
        "__pickleStuff(\"./data/sorted_summaries.p\",sorted_summaries)\n",
        "__pickleStuff(\"./data/sorted_texts.p\",sorted_texts)\n",
        "__pickleStuff(\"./data/word_embedding_matrix.p\",word_embedding_matrix)\n",
        "\n",
        "__pickleStuff(\"./data/vocab_to_int.p\",vocab_to_int)\n",
        "__pickleStuff(\"./data/int_to_vocab.p\",int_to_vocab)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-9a91da11c8fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m__pickleStuff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/clean_summaries.p\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclean_summaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m__pickleStuff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/clean_texts.p\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclean_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__pickleStuff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/sorted_summaries.p\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msorted_summaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__pickleStuff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/sorted_texts.p\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msorted_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-4e182fea8dd1>\u001b[0m in \u001b[0;36m__pickleStuff\u001b[0;34m(filename, stuff)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__pickleStuff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstuff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msave_stuff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstuff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_stuff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msave_stuff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/clean_summaries.p'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cSJbX_iJgYfV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_inputs():\n",
        "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
        "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
        "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
        "\n",
        "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HDOq8uRegYfX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_encoding_input(target_data, vocab_to_int, batch_size):  \n",
        "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1]) # slice it to target_data[0:batch_size, 0: -1]\n",
        "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0yNzF9FHgYfY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
        "    '''Create the encoding layer'''\n",
        "    layer_input = rnn_inputs\n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                    cell_bw, \n",
        "                                                                    layer_input,\n",
        "                                                                    sequence_length,\n",
        "                                                                    dtype=tf.float32)\n",
        "            layer_input = tf.concat(enc_output, 2)\n",
        "    # Join outputs since we are using a bidirectional RNN\n",
        "    enc_output = tf.concat(enc_output,2)\n",
        "    return enc_output, enc_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MB3FNgPQgYfa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, output_layer,\n",
        "                            vocab_size, max_summary_length,batch_size):\n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                        sequence_length=summary_length,\n",
        "                                                        time_major=False)\n",
        "\n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,\n",
        "                                                       helper=training_helper,\n",
        "                                                       initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n",
        "                                                       output_layer = output_layer)\n",
        "\n",
        "    training_logits = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                           output_time_major=False,\n",
        "                                                           impute_finished=True,\n",
        "                                                           maximum_iterations=max_summary_length)\n",
        "    return training_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2luruBcXgYfb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, output_layer,\n",
        "                             max_summary_length, batch_size):\n",
        "    '''Create the inference logits'''\n",
        "    \n",
        "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "    \n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                start_tokens,\n",
        "                                                                end_token)\n",
        "                \n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                        inference_helper,\n",
        "                                                        dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n",
        "                                                        output_layer)\n",
        "                \n",
        "    inference_logits = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                            output_time_major=False,\n",
        "                                                            impute_finished=True,\n",
        "                                                            maximum_iterations=max_summary_length)\n",
        "    \n",
        "    return inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9so6Maq6gYfd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lstm_cell(lstm_size, keep_prob):\n",
        "    cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
        "    return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)\n",
        "\n",
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\n",
        "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
        "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
        "    dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
        "    output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                     enc_output,\n",
        "                                                     text_length,\n",
        "                                                     normalize=False,\n",
        "                                                     name='BahdanauAttention')\n",
        "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,attn_mech,rnn_size)\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_logits = training_decoding_layer(dec_embed_input,summary_length,dec_cell,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size,\n",
        "                                                  max_summary_length,\n",
        "                                                  batch_size)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_logits = inference_decoding_layer(embeddings,\n",
        "                                                    vocab_to_int['<GO>'],\n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell,\n",
        "                                                    output_layer,\n",
        "                                                    max_summary_length,\n",
        "                                                    batch_size)\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0NAnCUo7gYfe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
        "    '''Use the previous functions to create the training and inference logits'''\n",
        "    \n",
        "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
        "    embeddings = word_embedding_matrix\n",
        "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
        "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size) #shape=(batch_size, senquence length) each seq start with index of<GO>\n",
        "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        text_length, \n",
        "                                                        summary_length, \n",
        "                                                        max_summary_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers)\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "06DlyJFygYfg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IlPQHeScgYfg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(summaries, texts, batch_size):\n",
        "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(texts)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
        "        texts_batch = texts[start_i:start_i + batch_size]\n",
        "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
        "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_summaries_lengths = []\n",
        "        for summary in pad_summaries_batch:\n",
        "            pad_summaries_lengths.append(len(summary))\n",
        "        \n",
        "        pad_texts_lengths = []\n",
        "        for text in pad_texts_batch:\n",
        "            pad_texts_lengths.append(len(text))\n",
        "        \n",
        "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ktdR2K57gYfh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "a8f5256e-9bcc-4fd4-f42b-089d819dd1c5"
      },
      "cell_type": "code",
      "source": [
        "print(\"'<PAD>' has id: {}\".format(vocab_to_int['<PAD>']))\n",
        "sorted_summaries_samples = sorted_summaries[7:50]\n",
        "sorted_texts_samples = sorted_texts[7:50]\n",
        "pad_summaries_batch_samples, pad_texts_batch_samples, pad_summaries_lengths_samples, pad_texts_lengths_samples = next(get_batches(\n",
        "    sorted_summaries_samples, sorted_texts_samples, 5))\n",
        "print(\"pad summaries batch samples:\\n\\r {}\".format(pad_summaries_batch_samples))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'<PAD>' has id: 65160\n",
            "pad summaries batch samples:\n",
            "\r [[  200   225   174   373 65160 65160 65160 65160 65160 65160 65160 65160\n",
            "  65160]\n",
            " [  200   225   174   373 65160 65160 65160 65160 65160 65160 65160 65160\n",
            "  65160]\n",
            " [ 3507    89    67   901    67  1431   432   159   317   231 65160 65160\n",
            "  65160]\n",
            " [ 5873  5874   560   561   602   485  2441  1940   677   109    41   330\n",
            "  65160]\n",
            " [  181  1128   560   561   877  3112  2005  2441  1940   677   109    41\n",
            "    330]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xl3H2fF3gYfj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set the Hyperparameters\n",
        "epochs = 150\n",
        "batch_size = 64\n",
        "rnn_size = 256\n",
        "num_layers = 2\n",
        "learning_rate = 0.005\n",
        "keep_probability = 0.95"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5w5WcYK-gYfl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8bdae471-7aa3-4bb9-890e-52f0e54296f4"
      },
      "cell_type": "code",
      "source": [
        "# Build the graph\n",
        "train_graph = tf.Graph()\n",
        "# Set the graph to default to ensure that it is ready for training\n",
        "with train_graph.as_default():\n",
        "    \n",
        "    # Load the model inputs    \n",
        "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      text_length,\n",
        "                                                      summary_length,\n",
        "                                                      max_summary_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size)\n",
        "    \n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits[0].rnn_output, 'logits')\n",
        "    inference_logits = tf.identity(inference_logits[0].sample_id, name='predictions')\n",
        "    \n",
        "    # Create the weights for sequence_loss, the sould be all True across since each batch is padded\n",
        "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "print(\"Graph is built.\")\n",
        "graph_location = \"./graph\"\n",
        "print(graph_location)\n",
        "train_writer = tf.summary.FileWriter(graph_location)\n",
        "train_writer.add_graph(train_graph)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-31-e5ed514e649b>:2: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
            "Graph is built.\n",
            "./graph\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SCQ3LMHygYfm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b4fe7049-49b2-4435-9e2f-f3474ac8c5af"
      },
      "cell_type": "code",
      "source": [
        "# Subset the data for training\n",
        "start = 200000\n",
        "end = start + 50000\n",
        "sorted_summaries_short = sorted_summaries[start:end]\n",
        "sorted_texts_short = sorted_texts[start:end]\n",
        "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
        "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shortest text length: 51\n",
            "The longest text length: 62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5IxrkiFPgYfo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 20417
        },
        "outputId": "9cc4fe91-d61b-4dc7-d1bf-86c72de9eaed"
      },
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "learning_rate_decay = 0.95\n",
        "min_learning_rate = 0.0005\n",
        "display_step = 20 # Check training loss after every 20 batches\n",
        "stop_early = 0 \n",
        "stop = 20 # If the update loss does not decrease in 20 consecutive update checks, stop training\n",
        "per_epoch = 3 # Make 3 update checks per epoch\n",
        "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
        "\n",
        "update_loss = 0 \n",
        "batch_loss = 0\n",
        "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
        "\n",
        "checkpoint = \"./output/model.ckpt\" \n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # If we want to continue training a previous session\n",
        "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
        "    #loader.restore(sess, checkpoint)\n",
        "    \n",
        "    for epoch_i in range(1, epochs+1):\n",
        "        update_loss = 0\n",
        "        batch_loss = 0\n",
        "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
        "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
        "            start_time = time.time()\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: texts_batch,\n",
        "                 targets: summaries_batch,\n",
        "                 lr: learning_rate,\n",
        "                 summary_length: summaries_lengths,\n",
        "                 text_length: texts_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "            batch_loss += loss\n",
        "            update_loss += loss\n",
        "            end_time = time.time()\n",
        "            batch_time = end_time - start_time\n",
        "\n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                      .format(epoch_i,\n",
        "                              epochs, \n",
        "                              batch_i, \n",
        "                              len(sorted_texts_short) // batch_size, \n",
        "                              batch_loss / display_step, \n",
        "                              batch_time*display_step))\n",
        "                batch_loss = 0\n",
        "\n",
        "            if batch_i % update_check == 0 and batch_i > 0:\n",
        "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
        "                summary_update_loss.append(update_loss)\n",
        "                \n",
        "                # If the update loss is at a new minimum, save the model\n",
        "                if update_loss <= min(summary_update_loss):\n",
        "                    print('New Record!') \n",
        "                    stop_early = 0\n",
        "                    saver = tf.train.Saver() \n",
        "                    saver.save(sess, checkpoint)\n",
        "\n",
        "                else:\n",
        "                    print(\"No Improvement.\")\n",
        "                    stop_early += 1\n",
        "                    if stop_early == stop:\n",
        "                        break\n",
        "                update_loss = 0\n",
        "            \n",
        "                    \n",
        "        # Reduce learning rate, but not below its minimum value\n",
        "        learning_rate *= learning_rate_decay\n",
        "        if learning_rate < min_learning_rate:\n",
        "            learning_rate = min_learning_rate\n",
        "        \n",
        "        if stop_early == stop:\n",
        "            print(\"Stopping Training.\")\n",
        "            break"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   1/150 Batch   20/781 - Loss:  5.175, Seconds: 10.00\n",
            "Epoch   1/150 Batch   40/781 - Loss:  2.829, Seconds: 9.75\n",
            "Epoch   1/150 Batch   60/781 - Loss:  2.734, Seconds: 10.14\n",
            "Epoch   1/150 Batch   80/781 - Loss:  2.885, Seconds: 9.49\n",
            "Epoch   1/150 Batch  100/781 - Loss:  2.901, Seconds: 10.07\n",
            "Epoch   1/150 Batch  120/781 - Loss:  2.761, Seconds: 10.95\n",
            "Epoch   1/150 Batch  140/781 - Loss:  2.811, Seconds: 10.16\n",
            "Epoch   1/150 Batch  160/781 - Loss:  2.629, Seconds: 10.46\n",
            "Epoch   1/150 Batch  180/781 - Loss:  2.719, Seconds: 10.34\n",
            "Epoch   1/150 Batch  200/781 - Loss:  2.616, Seconds: 10.52\n",
            "Epoch   1/150 Batch  220/781 - Loss:  2.652, Seconds: 11.04\n",
            "Epoch   1/150 Batch  240/781 - Loss:  2.789, Seconds: 10.08\n",
            "Average loss for this update: 2.939\n",
            "New Record!\n",
            "Epoch   1/150 Batch  260/781 - Loss:  2.695, Seconds: 12.82\n",
            "Epoch   1/150 Batch  280/781 - Loss:  2.706, Seconds: 10.93\n",
            "Epoch   1/150 Batch  300/781 - Loss:  2.799, Seconds: 10.70\n",
            "Epoch   1/150 Batch  320/781 - Loss:  2.689, Seconds: 10.88\n",
            "Epoch   1/150 Batch  340/781 - Loss:  2.672, Seconds: 10.06\n",
            "Epoch   1/150 Batch  360/781 - Loss:  2.694, Seconds: 10.33\n",
            "Epoch   1/150 Batch  380/781 - Loss:  2.749, Seconds: 10.67\n",
            "Epoch   1/150 Batch  400/781 - Loss:  2.601, Seconds: 10.67\n",
            "Epoch   1/150 Batch  420/781 - Loss:  2.583, Seconds: 10.84\n",
            "Epoch   1/150 Batch  440/781 - Loss:  2.680, Seconds: 10.73\n",
            "Epoch   1/150 Batch  460/781 - Loss:  2.577, Seconds: 11.57\n",
            "Epoch   1/150 Batch  480/781 - Loss:  2.519, Seconds: 10.85\n",
            "Epoch   1/150 Batch  500/781 - Loss:  2.604, Seconds: 10.97\n",
            "Average loss for this update: 2.653\n",
            "New Record!\n",
            "Epoch   1/150 Batch  520/781 - Loss:  2.600, Seconds: 10.86\n",
            "Epoch   1/150 Batch  540/781 - Loss:  2.472, Seconds: 11.28\n",
            "Epoch   1/150 Batch  560/781 - Loss:  2.565, Seconds: 11.32\n",
            "Epoch   1/150 Batch  580/781 - Loss:  2.482, Seconds: 11.13\n",
            "Epoch   1/150 Batch  600/781 - Loss:  2.525, Seconds: 11.14\n",
            "Epoch   1/150 Batch  620/781 - Loss:  2.498, Seconds: 11.60\n",
            "Epoch   1/150 Batch  640/781 - Loss:  2.447, Seconds: 11.61\n",
            "Epoch   1/150 Batch  660/781 - Loss:  2.538, Seconds: 11.58\n",
            "Epoch   1/150 Batch  680/781 - Loss:  2.425, Seconds: 10.53\n",
            "Epoch   1/150 Batch  700/781 - Loss:  2.621, Seconds: 11.22\n",
            "Epoch   1/150 Batch  720/781 - Loss:  2.468, Seconds: 11.87\n",
            "Epoch   1/150 Batch  740/781 - Loss:  2.385, Seconds: 11.98\n",
            "Epoch   1/150 Batch  760/781 - Loss:  2.552, Seconds: 11.20\n",
            "Average loss for this update: 2.491\n",
            "New Record!\n",
            "Epoch   1/150 Batch  780/781 - Loss:  2.369, Seconds: 11.65\n",
            "Epoch   2/150 Batch   20/781 - Loss:  2.489, Seconds: 10.08\n",
            "Epoch   2/150 Batch   40/781 - Loss:  2.250, Seconds: 10.07\n",
            "Epoch   2/150 Batch   60/781 - Loss:  2.222, Seconds: 10.36\n",
            "Epoch   2/150 Batch   80/781 - Loss:  2.399, Seconds: 9.51\n",
            "Epoch   2/150 Batch  100/781 - Loss:  2.434, Seconds: 9.92\n",
            "Epoch   2/150 Batch  120/781 - Loss:  2.306, Seconds: 11.09\n",
            "Epoch   2/150 Batch  140/781 - Loss:  2.359, Seconds: 10.12\n",
            "Epoch   2/150 Batch  160/781 - Loss:  2.241, Seconds: 10.22\n",
            "Epoch   2/150 Batch  180/781 - Loss:  2.337, Seconds: 11.96\n",
            "Epoch   2/150 Batch  200/781 - Loss:  2.202, Seconds: 10.67\n",
            "Epoch   2/150 Batch  220/781 - Loss:  2.267, Seconds: 10.96\n",
            "Epoch   2/150 Batch  240/781 - Loss:  2.419, Seconds: 9.99\n",
            "Average loss for this update: 2.325\n",
            "New Record!\n",
            "Epoch   2/150 Batch  260/781 - Loss:  2.306, Seconds: 12.05\n",
            "Epoch   2/150 Batch  280/781 - Loss:  2.317, Seconds: 10.74\n",
            "Epoch   2/150 Batch  300/781 - Loss:  2.422, Seconds: 10.58\n",
            "Epoch   2/150 Batch  320/781 - Loss:  2.316, Seconds: 10.80\n",
            "Epoch   2/150 Batch  340/781 - Loss:  2.283, Seconds: 10.09\n",
            "Epoch   2/150 Batch  360/781 - Loss:  2.332, Seconds: 10.65\n",
            "Epoch   2/150 Batch  380/781 - Loss:  2.393, Seconds: 10.85\n",
            "Epoch   2/150 Batch  400/781 - Loss:  2.267, Seconds: 10.65\n",
            "Epoch   2/150 Batch  420/781 - Loss:  2.242, Seconds: 11.23\n",
            "Epoch   2/150 Batch  440/781 - Loss:  2.356, Seconds: 10.75\n",
            "Epoch   2/150 Batch  460/781 - Loss:  2.247, Seconds: 11.64\n",
            "Epoch   2/150 Batch  480/781 - Loss:  2.188, Seconds: 11.00\n",
            "Epoch   2/150 Batch  500/781 - Loss:  2.285, Seconds: 11.17\n",
            "Average loss for this update: 2.303\n",
            "New Record!\n",
            "Epoch   2/150 Batch  520/781 - Loss:  2.274, Seconds: 11.64\n",
            "Epoch   2/150 Batch  540/781 - Loss:  2.152, Seconds: 11.54\n",
            "Epoch   2/150 Batch  560/781 - Loss:  2.259, Seconds: 10.69\n",
            "Epoch   2/150 Batch  580/781 - Loss:  2.211, Seconds: 11.29\n",
            "Epoch   2/150 Batch  600/781 - Loss:  2.205, Seconds: 10.84\n",
            "Epoch   2/150 Batch  620/781 - Loss:  2.217, Seconds: 11.45\n",
            "Epoch   2/150 Batch  640/781 - Loss:  2.190, Seconds: 11.30\n",
            "Epoch   2/150 Batch  660/781 - Loss:  2.256, Seconds: 11.79\n",
            "Epoch   2/150 Batch  680/781 - Loss:  2.147, Seconds: 10.39\n",
            "Epoch   2/150 Batch  700/781 - Loss:  2.337, Seconds: 11.15\n",
            "Epoch   2/150 Batch  720/781 - Loss:  2.208, Seconds: 11.79\n",
            "Epoch   2/150 Batch  740/781 - Loss:  2.125, Seconds: 11.88\n",
            "Epoch   2/150 Batch  760/781 - Loss:  2.292, Seconds: 10.86\n",
            "Average loss for this update: 2.212\n",
            "New Record!\n",
            "Epoch   2/150 Batch  780/781 - Loss:  2.127, Seconds: 11.60\n",
            "Epoch   3/150 Batch   20/781 - Loss:  2.286, Seconds: 10.03\n",
            "Epoch   3/150 Batch   40/781 - Loss:  2.050, Seconds: 10.02\n",
            "Epoch   3/150 Batch   60/781 - Loss:  2.001, Seconds: 10.44\n",
            "Epoch   3/150 Batch   80/781 - Loss:  2.182, Seconds: 9.66\n",
            "Epoch   3/150 Batch  100/781 - Loss:  2.222, Seconds: 10.52\n",
            "Epoch   3/150 Batch  120/781 - Loss:  2.079, Seconds: 10.96\n",
            "Epoch   3/150 Batch  140/781 - Loss:  2.144, Seconds: 10.10\n",
            "Epoch   3/150 Batch  160/781 - Loss:  2.070, Seconds: 10.44\n",
            "Epoch   3/150 Batch  180/781 - Loss:  2.152, Seconds: 10.36\n",
            "Epoch   3/150 Batch  200/781 - Loss:  1.987, Seconds: 10.54\n",
            "Epoch   3/150 Batch  220/781 - Loss:  2.060, Seconds: 10.94\n",
            "Epoch   3/150 Batch  240/781 - Loss:  2.220, Seconds: 10.02\n",
            "Average loss for this update: 2.121\n",
            "New Record!\n",
            "Epoch   3/150 Batch  260/781 - Loss:  2.120, Seconds: 11.67\n",
            "Epoch   3/150 Batch  280/781 - Loss:  2.107, Seconds: 11.22\n",
            "Epoch   3/150 Batch  300/781 - Loss:  2.240, Seconds: 10.77\n",
            "Epoch   3/150 Batch  320/781 - Loss:  2.113, Seconds: 10.41\n",
            "Epoch   3/150 Batch  340/781 - Loss:  2.052, Seconds: 10.16\n",
            "Epoch   3/150 Batch  360/781 - Loss:  2.124, Seconds: 10.65\n",
            "Epoch   3/150 Batch  380/781 - Loss:  2.194, Seconds: 10.77\n",
            "Epoch   3/150 Batch  400/781 - Loss:  2.057, Seconds: 10.60\n",
            "Epoch   3/150 Batch  420/781 - Loss:  2.018, Seconds: 11.26\n",
            "Epoch   3/150 Batch  440/781 - Loss:  2.182, Seconds: 11.00\n",
            "Epoch   3/150 Batch  460/781 - Loss:  2.017, Seconds: 11.55\n",
            "Epoch   3/150 Batch  480/781 - Loss:  1.970, Seconds: 10.80\n",
            "Epoch   3/150 Batch  500/781 - Loss:  2.096, Seconds: 11.37\n",
            "Average loss for this update: 2.095\n",
            "New Record!\n",
            "Epoch   3/150 Batch  520/781 - Loss:  2.061, Seconds: 11.78\n",
            "Epoch   3/150 Batch  540/781 - Loss:  2.010, Seconds: 11.46\n",
            "Epoch   3/150 Batch  560/781 - Loss:  2.096, Seconds: 10.68\n",
            "Epoch   3/150 Batch  580/781 - Loss:  2.060, Seconds: 11.13\n",
            "Epoch   3/150 Batch  600/781 - Loss:  2.022, Seconds: 11.45\n",
            "Epoch   3/150 Batch  620/781 - Loss:  2.044, Seconds: 11.58\n",
            "Epoch   3/150 Batch  640/781 - Loss:  2.041, Seconds: 11.58\n",
            "Epoch   3/150 Batch  660/781 - Loss:  2.069, Seconds: 11.59\n",
            "Epoch   3/150 Batch  680/781 - Loss:  1.950, Seconds: 10.35\n",
            "Epoch   3/150 Batch  700/781 - Loss:  2.178, Seconds: 11.19\n",
            "Epoch   3/150 Batch  720/781 - Loss:  2.052, Seconds: 11.73\n",
            "Epoch   3/150 Batch  740/781 - Loss:  1.953, Seconds: 12.17\n",
            "Epoch   3/150 Batch  760/781 - Loss:  2.128, Seconds: 11.00\n",
            "Average loss for this update: 2.046\n",
            "New Record!\n",
            "Epoch   3/150 Batch  780/781 - Loss:  1.969, Seconds: 12.39\n",
            "Epoch   4/150 Batch   20/781 - Loss:  2.131, Seconds: 10.12\n",
            "Epoch   4/150 Batch   40/781 - Loss:  1.876, Seconds: 10.21\n",
            "Epoch   4/150 Batch   60/781 - Loss:  1.830, Seconds: 10.38\n",
            "Epoch   4/150 Batch   80/781 - Loss:  2.006, Seconds: 9.80\n",
            "Epoch   4/150 Batch  100/781 - Loss:  2.056, Seconds: 10.44\n",
            "Epoch   4/150 Batch  120/781 - Loss:  1.925, Seconds: 11.07\n",
            "Epoch   4/150 Batch  140/781 - Loss:  1.981, Seconds: 10.47\n",
            "Epoch   4/150 Batch  160/781 - Loss:  1.929, Seconds: 10.37\n",
            "Epoch   4/150 Batch  180/781 - Loss:  1.989, Seconds: 10.37\n",
            "Epoch   4/150 Batch  200/781 - Loss:  1.823, Seconds: 10.96\n",
            "Epoch   4/150 Batch  220/781 - Loss:  1.905, Seconds: 11.54\n",
            "Epoch   4/150 Batch  240/781 - Loss:  2.099, Seconds: 10.02\n",
            "Average loss for this update: 1.964\n",
            "New Record!\n",
            "Epoch   4/150 Batch  260/781 - Loss:  1.988, Seconds: 13.04\n",
            "Epoch   4/150 Batch  280/781 - Loss:  1.968, Seconds: 11.08\n",
            "Epoch   4/150 Batch  300/781 - Loss:  2.098, Seconds: 10.89\n",
            "Epoch   4/150 Batch  320/781 - Loss:  1.971, Seconds: 10.58\n",
            "Epoch   4/150 Batch  340/781 - Loss:  1.874, Seconds: 10.16\n",
            "Epoch   4/150 Batch  360/781 - Loss:  1.970, Seconds: 10.59\n",
            "Epoch   4/150 Batch  380/781 - Loss:  2.048, Seconds: 10.75\n",
            "Epoch   4/150 Batch  400/781 - Loss:  1.905, Seconds: 10.76\n",
            "Epoch   4/150 Batch  420/781 - Loss:  1.863, Seconds: 11.21\n",
            "Epoch   4/150 Batch  440/781 - Loss:  2.025, Seconds: 11.18\n",
            "Epoch   4/150 Batch  460/781 - Loss:  1.872, Seconds: 11.46\n",
            "Epoch   4/150 Batch  480/781 - Loss:  1.804, Seconds: 10.81\n",
            "Epoch   4/150 Batch  500/781 - Loss:  1.957, Seconds: 11.19\n",
            "Average loss for this update: 1.943\n",
            "New Record!\n",
            "Epoch   4/150 Batch  520/781 - Loss:  1.898, Seconds: 11.61\n",
            "Epoch   4/150 Batch  540/781 - Loss:  1.795, Seconds: 11.76\n",
            "Epoch   4/150 Batch  560/781 - Loss:  1.901, Seconds: 10.92\n",
            "Epoch   4/150 Batch  580/781 - Loss:  1.930, Seconds: 11.45\n",
            "Epoch   4/150 Batch  600/781 - Loss:  1.883, Seconds: 11.10\n",
            "Epoch   4/150 Batch  620/781 - Loss:  1.892, Seconds: 11.89\n",
            "Epoch   4/150 Batch  640/781 - Loss:  1.896, Seconds: 11.56\n",
            "Epoch   4/150 Batch  660/781 - Loss:  1.909, Seconds: 11.67\n",
            "Epoch   4/150 Batch  680/781 - Loss:  1.795, Seconds: 10.53\n",
            "Epoch   4/150 Batch  700/781 - Loss:  2.022, Seconds: 11.11\n",
            "Epoch   4/150 Batch  720/781 - Loss:  1.918, Seconds: 11.68\n",
            "Epoch   4/150 Batch  740/781 - Loss:  1.822, Seconds: 11.81\n",
            "Epoch   4/150 Batch  760/781 - Loss:  1.980, Seconds: 11.21\n",
            "Average loss for this update: 1.893\n",
            "New Record!\n",
            "Epoch   4/150 Batch  780/781 - Loss:  1.826, Seconds: 11.54\n",
            "Epoch   5/150 Batch   20/781 - Loss:  1.983, Seconds: 10.20\n",
            "Epoch   5/150 Batch   40/781 - Loss:  1.745, Seconds: 10.09\n",
            "Epoch   5/150 Batch   60/781 - Loss:  1.689, Seconds: 10.09\n",
            "Epoch   5/150 Batch   80/781 - Loss:  1.865, Seconds: 9.63\n",
            "Epoch   5/150 Batch  100/781 - Loss:  1.916, Seconds: 10.16\n",
            "Epoch   5/150 Batch  120/781 - Loss:  1.798, Seconds: 10.99\n",
            "Epoch   5/150 Batch  140/781 - Loss:  1.840, Seconds: 9.90\n",
            "Epoch   5/150 Batch  160/781 - Loss:  1.813, Seconds: 10.01\n",
            "Epoch   5/150 Batch  180/781 - Loss:  1.842, Seconds: 10.17\n",
            "Epoch   5/150 Batch  200/781 - Loss:  1.679, Seconds: 10.58\n",
            "Epoch   5/150 Batch  220/781 - Loss:  1.782, Seconds: 10.86\n",
            "Epoch   5/150 Batch  240/781 - Loss:  1.993, Seconds: 10.34\n",
            "Average loss for this update: 1.83\n",
            "New Record!\n",
            "Epoch   5/150 Batch  260/781 - Loss:  1.855, Seconds: 12.88\n",
            "Epoch   5/150 Batch  280/781 - Loss:  1.850, Seconds: 10.94\n",
            "Epoch   5/150 Batch  300/781 - Loss:  1.966, Seconds: 10.79\n",
            "Epoch   5/150 Batch  320/781 - Loss:  1.842, Seconds: 10.71\n",
            "Epoch   5/150 Batch  340/781 - Loss:  1.726, Seconds: 10.38\n",
            "Epoch   5/150 Batch  360/781 - Loss:  1.859, Seconds: 10.31\n",
            "Epoch   5/150 Batch  380/781 - Loss:  1.928, Seconds: 10.71\n",
            "Epoch   5/150 Batch  400/781 - Loss:  1.783, Seconds: 10.56\n",
            "Epoch   5/150 Batch  420/781 - Loss:  1.742, Seconds: 11.39\n",
            "Epoch   5/150 Batch  440/781 - Loss:  1.906, Seconds: 10.79\n",
            "Epoch   5/150 Batch  460/781 - Loss:  1.765, Seconds: 11.36\n",
            "Epoch   5/150 Batch  480/781 - Loss:  1.695, Seconds: 10.94\n",
            "Epoch   5/150 Batch  500/781 - Loss:  1.846, Seconds: 11.22\n",
            "Average loss for this update: 1.823\n",
            "New Record!\n",
            "Epoch   5/150 Batch  520/781 - Loss:  1.774, Seconds: 11.56\n",
            "Epoch   5/150 Batch  540/781 - Loss:  1.671, Seconds: 11.65\n",
            "Epoch   5/150 Batch  560/781 - Loss:  1.786, Seconds: 11.03\n",
            "Epoch   5/150 Batch  580/781 - Loss:  1.834, Seconds: 11.13\n",
            "Epoch   5/150 Batch  600/781 - Loss:  1.763, Seconds: 11.37\n",
            "Epoch   5/150 Batch  620/781 - Loss:  1.789, Seconds: 11.78\n",
            "Epoch   5/150 Batch  640/781 - Loss:  1.777, Seconds: 11.52\n",
            "Epoch   5/150 Batch  660/781 - Loss:  1.788, Seconds: 11.84\n",
            "Epoch   5/150 Batch  680/781 - Loss:  1.696, Seconds: 10.50\n",
            "Epoch   5/150 Batch  700/781 - Loss:  1.910, Seconds: 11.09\n",
            "Epoch   5/150 Batch  720/781 - Loss:  1.810, Seconds: 12.12\n",
            "Epoch   5/150 Batch  740/781 - Loss:  1.716, Seconds: 12.21\n",
            "Epoch   5/150 Batch  760/781 - Loss:  1.879, Seconds: 11.46\n",
            "Average loss for this update: 1.783\n",
            "New Record!\n",
            "Epoch   5/150 Batch  780/781 - Loss:  1.734, Seconds: 11.49\n",
            "Epoch   6/150 Batch   20/781 - Loss:  1.859, Seconds: 10.43\n",
            "Epoch   6/150 Batch   40/781 - Loss:  1.657, Seconds: 10.02\n",
            "Epoch   6/150 Batch   60/781 - Loss:  1.597, Seconds: 10.39\n",
            "Epoch   6/150 Batch   80/781 - Loss:  1.780, Seconds: 9.80\n",
            "Epoch   6/150 Batch  100/781 - Loss:  1.804, Seconds: 10.01\n",
            "Epoch   6/150 Batch  120/781 - Loss:  1.703, Seconds: 10.53\n",
            "Epoch   6/150 Batch  140/781 - Loss:  1.705, Seconds: 10.26\n",
            "Epoch   6/150 Batch  160/781 - Loss:  1.704, Seconds: 10.53\n",
            "Epoch   6/150 Batch  180/781 - Loss:  1.747, Seconds: 10.34\n",
            "Epoch   6/150 Batch  200/781 - Loss:  1.579, Seconds: 10.64\n",
            "Epoch   6/150 Batch  220/781 - Loss:  1.686, Seconds: 11.11\n",
            "Epoch   6/150 Batch  240/781 - Loss:  1.905, Seconds: 10.45\n",
            "Average loss for this update: 1.731\n",
            "New Record!\n",
            "Epoch   6/150 Batch  260/781 - Loss:  1.777, Seconds: 12.45\n",
            "Epoch   6/150 Batch  280/781 - Loss:  1.759, Seconds: 10.97\n",
            "Epoch   6/150 Batch  300/781 - Loss:  1.879, Seconds: 11.10\n",
            "Epoch   6/150 Batch  320/781 - Loss:  1.743, Seconds: 10.84\n",
            "Epoch   6/150 Batch  340/781 - Loss:  1.624, Seconds: 10.19\n",
            "Epoch   6/150 Batch  360/781 - Loss:  1.766, Seconds: 10.60\n",
            "Epoch   6/150 Batch  380/781 - Loss:  1.830, Seconds: 10.81\n",
            "Epoch   6/150 Batch  400/781 - Loss:  1.687, Seconds: 10.73\n",
            "Epoch   6/150 Batch  420/781 - Loss:  1.645, Seconds: 11.27\n",
            "Epoch   6/150 Batch  440/781 - Loss:  1.810, Seconds: 11.00\n",
            "Epoch   6/150 Batch  460/781 - Loss:  1.689, Seconds: 11.19\n",
            "Epoch   6/150 Batch  480/781 - Loss:  1.612, Seconds: 10.93\n",
            "Epoch   6/150 Batch  500/781 - Loss:  1.749, Seconds: 11.36\n",
            "Average loss for this update: 1.729\n",
            "New Record!\n",
            "Epoch   6/150 Batch  520/781 - Loss:  1.675, Seconds: 11.28\n",
            "Epoch   6/150 Batch  540/781 - Loss:  1.568, Seconds: 11.85\n",
            "Epoch   6/150 Batch  560/781 - Loss:  1.689, Seconds: 10.83\n",
            "Epoch   6/150 Batch  580/781 - Loss:  1.735, Seconds: 11.06\n",
            "Epoch   6/150 Batch  600/781 - Loss:  1.664, Seconds: 11.09\n",
            "Epoch   6/150 Batch  620/781 - Loss:  1.707, Seconds: 11.64\n",
            "Epoch   6/150 Batch  640/781 - Loss:  1.694, Seconds: 11.73\n",
            "Epoch   6/150 Batch  660/781 - Loss:  1.698, Seconds: 11.61\n",
            "Epoch   6/150 Batch  680/781 - Loss:  1.605, Seconds: 10.48\n",
            "Epoch   6/150 Batch  700/781 - Loss:  1.819, Seconds: 10.91\n",
            "Epoch   6/150 Batch  720/781 - Loss:  1.708, Seconds: 11.88\n",
            "Epoch   6/150 Batch  740/781 - Loss:  1.629, Seconds: 12.00\n",
            "Epoch   6/150 Batch  760/781 - Loss:  1.788, Seconds: 11.17\n",
            "Average loss for this update: 1.69\n",
            "New Record!\n",
            "Epoch   6/150 Batch  780/781 - Loss:  1.634, Seconds: 11.56\n",
            "Epoch   7/150 Batch   20/781 - Loss:  1.778, Seconds: 9.91\n",
            "Epoch   7/150 Batch   40/781 - Loss:  1.592, Seconds: 10.21\n",
            "Epoch   7/150 Batch   60/781 - Loss:  1.508, Seconds: 10.31\n",
            "Epoch   7/150 Batch   80/781 - Loss:  1.719, Seconds: 9.76\n",
            "Epoch   7/150 Batch  100/781 - Loss:  1.695, Seconds: 10.36\n",
            "Epoch   7/150 Batch  120/781 - Loss:  1.612, Seconds: 10.86\n",
            "Epoch   7/150 Batch  140/781 - Loss:  1.637, Seconds: 10.72\n",
            "Epoch   7/150 Batch  160/781 - Loss:  1.617, Seconds: 10.43\n",
            "Epoch   7/150 Batch  180/781 - Loss:  1.666, Seconds: 10.19\n",
            "Epoch   7/150 Batch  200/781 - Loss:  1.491, Seconds: 10.54\n",
            "Epoch   7/150 Batch  220/781 - Loss:  1.688, Seconds: 10.77\n",
            "Epoch   7/150 Batch  240/781 - Loss:  1.903, Seconds: 9.90\n",
            "Average loss for this update: 1.666\n",
            "New Record!\n",
            "Epoch   7/150 Batch  260/781 - Loss:  1.761, Seconds: 12.36\n",
            "Epoch   7/150 Batch  280/781 - Loss:  1.729, Seconds: 11.02\n",
            "Epoch   7/150 Batch  300/781 - Loss:  1.946, Seconds: 10.87\n",
            "Epoch   7/150 Batch  320/781 - Loss:  1.771, Seconds: 10.58\n",
            "Epoch   7/150 Batch  340/781 - Loss:  1.635, Seconds: 10.30\n",
            "Epoch   7/150 Batch  360/781 - Loss:  1.754, Seconds: 10.26\n",
            "Epoch   7/150 Batch  380/781 - Loss:  1.805, Seconds: 10.61\n",
            "Epoch   7/150 Batch  400/781 - Loss:  1.692, Seconds: 10.67\n",
            "Epoch   7/150 Batch  420/781 - Loss:  1.676, Seconds: 11.05\n",
            "Epoch   7/150 Batch  440/781 - Loss:  1.793, Seconds: 10.53\n",
            "Epoch   7/150 Batch  460/781 - Loss:  1.676, Seconds: 11.28\n",
            "Epoch   7/150 Batch  480/781 - Loss:  1.607, Seconds: 10.71\n",
            "Epoch   7/150 Batch  500/781 - Loss:  1.710, Seconds: 11.32\n",
            "Average loss for this update: 1.727\n",
            "No Improvement.\n",
            "Epoch   7/150 Batch  520/781 - Loss:  1.644, Seconds: 10.95\n",
            "Epoch   7/150 Batch  540/781 - Loss:  1.550, Seconds: 11.47\n",
            "Epoch   7/150 Batch  560/781 - Loss:  1.646, Seconds: 10.78\n",
            "Epoch   7/150 Batch  580/781 - Loss:  1.689, Seconds: 10.97\n",
            "Epoch   7/150 Batch  600/781 - Loss:  1.612, Seconds: 12.09\n",
            "Epoch   7/150 Batch  620/781 - Loss:  1.662, Seconds: 11.78\n",
            "Epoch   7/150 Batch  640/781 - Loss:  1.637, Seconds: 11.63\n",
            "Epoch   7/150 Batch  660/781 - Loss:  1.647, Seconds: 11.55\n",
            "Epoch   7/150 Batch  680/781 - Loss:  1.556, Seconds: 10.29\n",
            "Epoch   7/150 Batch  700/781 - Loss:  1.767, Seconds: 10.90\n",
            "Epoch   7/150 Batch  720/781 - Loss:  1.664, Seconds: 11.69\n",
            "Epoch   7/150 Batch  740/781 - Loss:  1.565, Seconds: 11.77\n",
            "Epoch   7/150 Batch  760/781 - Loss:  1.754, Seconds: 11.13\n",
            "Average loss for this update: 1.644\n",
            "New Record!\n",
            "Epoch   7/150 Batch  780/781 - Loss:  1.592, Seconds: 11.65\n",
            "Epoch   8/150 Batch   20/781 - Loss:  1.720, Seconds: 10.27\n",
            "Epoch   8/150 Batch   40/781 - Loss:  1.543, Seconds: 10.19\n",
            "Epoch   8/150 Batch   60/781 - Loss:  1.448, Seconds: 10.62\n",
            "Epoch   8/150 Batch   80/781 - Loss:  1.648, Seconds: 9.68\n",
            "Epoch   8/150 Batch  100/781 - Loss:  1.646, Seconds: 10.26\n",
            "Epoch   8/150 Batch  120/781 - Loss:  1.550, Seconds: 10.91\n",
            "Epoch   8/150 Batch  140/781 - Loss:  1.593, Seconds: 10.17\n",
            "Epoch   8/150 Batch  160/781 - Loss:  1.574, Seconds: 10.28\n",
            "Epoch   8/150 Batch  180/781 - Loss:  1.596, Seconds: 10.36\n",
            "Epoch   8/150 Batch  200/781 - Loss:  1.451, Seconds: 10.64\n",
            "Epoch   8/150 Batch  220/781 - Loss:  1.543, Seconds: 11.19\n",
            "Epoch   8/150 Batch  240/781 - Loss:  1.788, Seconds: 10.18\n",
            "Average loss for this update: 1.596\n",
            "New Record!\n",
            "Epoch   8/150 Batch  260/781 - Loss:  1.660, Seconds: 11.46\n",
            "Epoch   8/150 Batch  280/781 - Loss:  1.628, Seconds: 11.17\n",
            "Epoch   8/150 Batch  300/781 - Loss:  1.755, Seconds: 11.05\n",
            "Epoch   8/150 Batch  320/781 - Loss:  1.652, Seconds: 10.64\n",
            "Epoch   8/150 Batch  340/781 - Loss:  1.519, Seconds: 10.29\n",
            "Epoch   8/150 Batch  360/781 - Loss:  1.641, Seconds: 10.57\n",
            "Epoch   8/150 Batch  380/781 - Loss:  1.686, Seconds: 10.66\n",
            "Epoch   8/150 Batch  400/781 - Loss:  1.574, Seconds: 10.97\n",
            "Epoch   8/150 Batch  420/781 - Loss:  1.534, Seconds: 11.01\n",
            "Epoch   8/150 Batch  440/781 - Loss:  1.649, Seconds: 10.73\n",
            "Epoch   8/150 Batch  460/781 - Loss:  1.558, Seconds: 11.76\n",
            "Epoch   8/150 Batch  480/781 - Loss:  1.499, Seconds: 10.94\n",
            "Epoch   8/150 Batch  500/781 - Loss:  1.598, Seconds: 11.50\n",
            "Average loss for this update: 1.602\n",
            "No Improvement.\n",
            "Epoch   8/150 Batch  520/781 - Loss:  1.522, Seconds: 11.09\n",
            "Epoch   8/150 Batch  540/781 - Loss:  1.455, Seconds: 11.68\n",
            "Epoch   8/150 Batch  560/781 - Loss:  1.557, Seconds: 10.82\n",
            "Epoch   8/150 Batch  580/781 - Loss:  1.592, Seconds: 11.13\n",
            "Epoch   8/150 Batch  600/781 - Loss:  1.514, Seconds: 11.11\n",
            "Epoch   8/150 Batch  620/781 - Loss:  1.571, Seconds: 11.63\n",
            "Epoch   8/150 Batch  640/781 - Loss:  1.542, Seconds: 11.58\n",
            "Epoch   8/150 Batch  660/781 - Loss:  1.547, Seconds: 11.41\n",
            "Epoch   8/150 Batch  680/781 - Loss:  1.472, Seconds: 10.37\n",
            "Epoch   8/150 Batch  700/781 - Loss:  1.682, Seconds: 10.83\n",
            "Epoch   8/150 Batch  720/781 - Loss:  1.564, Seconds: 11.69\n",
            "Epoch   8/150 Batch  740/781 - Loss:  1.484, Seconds: 11.87\n",
            "Epoch   8/150 Batch  760/781 - Loss:  1.656, Seconds: 11.09\n",
            "Average loss for this update: 1.553\n",
            "New Record!\n",
            "Epoch   8/150 Batch  780/781 - Loss:  1.526, Seconds: 11.66\n",
            "Epoch   9/150 Batch   20/781 - Loss:  1.632, Seconds: 10.09\n",
            "Epoch   9/150 Batch   40/781 - Loss:  1.463, Seconds: 10.00\n",
            "Epoch   9/150 Batch   60/781 - Loss:  1.380, Seconds: 10.36\n",
            "Epoch   9/150 Batch   80/781 - Loss:  1.570, Seconds: 9.51\n",
            "Epoch   9/150 Batch  100/781 - Loss:  1.565, Seconds: 10.50\n",
            "Epoch   9/150 Batch  120/781 - Loss:  1.486, Seconds: 10.75\n",
            "Epoch   9/150 Batch  140/781 - Loss:  1.512, Seconds: 10.18\n",
            "Epoch   9/150 Batch  160/781 - Loss:  1.492, Seconds: 10.40\n",
            "Epoch   9/150 Batch  180/781 - Loss:  1.511, Seconds: 10.27\n",
            "Epoch   9/150 Batch  200/781 - Loss:  1.376, Seconds: 10.87\n",
            "Epoch   9/150 Batch  220/781 - Loss:  1.464, Seconds: 11.12\n",
            "Epoch   9/150 Batch  240/781 - Loss:  1.697, Seconds: 10.16\n",
            "Average loss for this update: 1.517\n",
            "New Record!\n",
            "Epoch   9/150 Batch  260/781 - Loss:  1.573, Seconds: 12.42\n",
            "Epoch   9/150 Batch  280/781 - Loss:  1.539, Seconds: 11.19\n",
            "Epoch   9/150 Batch  300/781 - Loss:  1.665, Seconds: 10.98\n",
            "Epoch   9/150 Batch  320/781 - Loss:  1.571, Seconds: 10.53\n",
            "Epoch   9/150 Batch  340/781 - Loss:  1.437, Seconds: 10.27\n",
            "Epoch   9/150 Batch  360/781 - Loss:  1.545, Seconds: 10.49\n",
            "Epoch   9/150 Batch  380/781 - Loss:  1.593, Seconds: 10.75\n",
            "Epoch   9/150 Batch  400/781 - Loss:  1.484, Seconds: 10.93\n",
            "Epoch   9/150 Batch  420/781 - Loss:  1.466, Seconds: 11.30\n",
            "Epoch   9/150 Batch  440/781 - Loss:  1.583, Seconds: 11.01\n",
            "Epoch   9/150 Batch  460/781 - Loss:  1.485, Seconds: 11.59\n",
            "Epoch   9/150 Batch  480/781 - Loss:  1.437, Seconds: 10.93\n",
            "Epoch   9/150 Batch  500/781 - Loss:  1.530, Seconds: 11.45\n",
            "Average loss for this update: 1.523\n",
            "No Improvement.\n",
            "Epoch   9/150 Batch  520/781 - Loss:  1.458, Seconds: 11.45\n",
            "Epoch   9/150 Batch  540/781 - Loss:  1.386, Seconds: 11.83\n",
            "Epoch   9/150 Batch  560/781 - Loss:  1.490, Seconds: 10.83\n",
            "Epoch   9/150 Batch  580/781 - Loss:  1.536, Seconds: 11.23\n",
            "Epoch   9/150 Batch  600/781 - Loss:  1.447, Seconds: 11.44\n",
            "Epoch   9/150 Batch  620/781 - Loss:  1.500, Seconds: 11.79\n",
            "Epoch   9/150 Batch  640/781 - Loss:  1.481, Seconds: 11.79\n",
            "Epoch   9/150 Batch  660/781 - Loss:  1.480, Seconds: 11.84\n",
            "Epoch   9/150 Batch  680/781 - Loss:  1.407, Seconds: 10.52\n",
            "Epoch   9/150 Batch  700/781 - Loss:  1.620, Seconds: 11.17\n",
            "Epoch   9/150 Batch  720/781 - Loss:  1.505, Seconds: 11.72\n",
            "Epoch   9/150 Batch  740/781 - Loss:  1.412, Seconds: 11.72\n",
            "Epoch   9/150 Batch  760/781 - Loss:  1.585, Seconds: 11.14\n",
            "Average loss for this update: 1.488\n",
            "New Record!\n",
            "Epoch   9/150 Batch  780/781 - Loss:  1.479, Seconds: 11.58\n",
            "Epoch  10/150 Batch   20/781 - Loss:  1.577, Seconds: 10.45\n",
            "Epoch  10/150 Batch   40/781 - Loss:  1.410, Seconds: 10.16\n",
            "Epoch  10/150 Batch   60/781 - Loss:  1.335, Seconds: 10.58\n",
            "Epoch  10/150 Batch   80/781 - Loss:  1.507, Seconds: 9.69\n",
            "Epoch  10/150 Batch  100/781 - Loss:  1.494, Seconds: 10.20\n",
            "Epoch  10/150 Batch  120/781 - Loss:  1.418, Seconds: 11.01\n",
            "Epoch  10/150 Batch  140/781 - Loss:  1.448, Seconds: 10.06\n",
            "Epoch  10/150 Batch  160/781 - Loss:  1.444, Seconds: 10.31\n",
            "Epoch  10/150 Batch  180/781 - Loss:  1.448, Seconds: 10.33\n",
            "Epoch  10/150 Batch  200/781 - Loss:  1.332, Seconds: 10.82\n",
            "Epoch  10/150 Batch  220/781 - Loss:  1.409, Seconds: 11.26\n",
            "Epoch  10/150 Batch  240/781 - Loss:  1.619, Seconds: 10.29\n",
            "Average loss for this update: 1.457\n",
            "New Record!\n",
            "Epoch  10/150 Batch  260/781 - Loss:  1.503, Seconds: 12.22\n",
            "Epoch  10/150 Batch  280/781 - Loss:  1.474, Seconds: 11.33\n",
            "Epoch  10/150 Batch  300/781 - Loss:  1.599, Seconds: 10.69\n",
            "Epoch  10/150 Batch  320/781 - Loss:  1.490, Seconds: 10.89\n",
            "Epoch  10/150 Batch  340/781 - Loss:  1.368, Seconds: 10.26\n",
            "Epoch  10/150 Batch  360/781 - Loss:  1.484, Seconds: 10.98\n",
            "Epoch  10/150 Batch  380/781 - Loss:  1.525, Seconds: 11.79\n",
            "Epoch  10/150 Batch  400/781 - Loss:  1.416, Seconds: 11.12\n",
            "Epoch  10/150 Batch  420/781 - Loss:  1.403, Seconds: 11.17\n",
            "Epoch  10/150 Batch  440/781 - Loss:  1.517, Seconds: 11.02\n",
            "Epoch  10/150 Batch  460/781 - Loss:  1.420, Seconds: 11.54\n",
            "Epoch  10/150 Batch  480/781 - Loss:  1.376, Seconds: 10.77\n",
            "Epoch  10/150 Batch  500/781 - Loss:  1.468, Seconds: 12.36\n",
            "Average loss for this update: 1.457\n",
            "New Record!\n",
            "Epoch  10/150 Batch  520/781 - Loss:  1.396, Seconds: 11.15\n",
            "Epoch  10/150 Batch  540/781 - Loss:  1.337, Seconds: 11.64\n",
            "Epoch  10/150 Batch  560/781 - Loss:  1.437, Seconds: 11.05\n",
            "Epoch  10/150 Batch  580/781 - Loss:  1.479, Seconds: 11.16\n",
            "Epoch  10/150 Batch  600/781 - Loss:  1.385, Seconds: 11.13\n",
            "Epoch  10/150 Batch  620/781 - Loss:  1.452, Seconds: 11.68\n",
            "Epoch  10/150 Batch  640/781 - Loss:  1.425, Seconds: 11.73\n",
            "Epoch  10/150 Batch  660/781 - Loss:  1.417, Seconds: 11.50\n",
            "Epoch  10/150 Batch  680/781 - Loss:  1.351, Seconds: 10.09\n",
            "Epoch  10/150 Batch  700/781 - Loss:  1.555, Seconds: 11.10\n",
            "Epoch  10/150 Batch  720/781 - Loss:  1.456, Seconds: 11.58\n",
            "Epoch  10/150 Batch  740/781 - Loss:  1.348, Seconds: 11.66\n",
            "Epoch  10/150 Batch  760/781 - Loss:  1.527, Seconds: 11.08\n",
            "Average loss for this update: 1.432\n",
            "New Record!\n",
            "Epoch  10/150 Batch  780/781 - Loss:  1.419, Seconds: 11.66\n",
            "Epoch  11/150 Batch   20/781 - Loss:  1.513, Seconds: 10.23\n",
            "Epoch  11/150 Batch   40/781 - Loss:  1.349, Seconds: 10.37\n",
            "Epoch  11/150 Batch   60/781 - Loss:  1.272, Seconds: 10.43\n",
            "Epoch  11/150 Batch   80/781 - Loss:  1.431, Seconds: 9.71\n",
            "Epoch  11/150 Batch  100/781 - Loss:  1.421, Seconds: 10.54\n",
            "Epoch  11/150 Batch  120/781 - Loss:  1.353, Seconds: 10.73\n",
            "Epoch  11/150 Batch  140/781 - Loss:  1.379, Seconds: 10.39\n",
            "Epoch  11/150 Batch  160/781 - Loss:  1.383, Seconds: 10.58\n",
            "Epoch  11/150 Batch  180/781 - Loss:  1.389, Seconds: 10.46\n",
            "Epoch  11/150 Batch  200/781 - Loss:  1.270, Seconds: 10.75\n",
            "Epoch  11/150 Batch  220/781 - Loss:  1.359, Seconds: 10.84\n",
            "Epoch  11/150 Batch  240/781 - Loss:  1.564, Seconds: 10.01\n",
            "Average loss for this update: 1.394\n",
            "New Record!\n",
            "Epoch  11/150 Batch  260/781 - Loss:  1.437, Seconds: 11.47\n",
            "Epoch  11/150 Batch  280/781 - Loss:  1.404, Seconds: 11.36\n",
            "Epoch  11/150 Batch  300/781 - Loss:  1.521, Seconds: 10.72\n",
            "Epoch  11/150 Batch  320/781 - Loss:  1.421, Seconds: 10.71\n",
            "Epoch  11/150 Batch  340/781 - Loss:  1.315, Seconds: 10.55\n",
            "Epoch  11/150 Batch  360/781 - Loss:  1.427, Seconds: 10.29\n",
            "Epoch  11/150 Batch  380/781 - Loss:  1.480, Seconds: 10.87\n",
            "Epoch  11/150 Batch  400/781 - Loss:  1.357, Seconds: 10.68\n",
            "Epoch  11/150 Batch  420/781 - Loss:  1.352, Seconds: 11.41\n",
            "Epoch  11/150 Batch  440/781 - Loss:  1.460, Seconds: 10.99\n",
            "Epoch  11/150 Batch  460/781 - Loss:  1.369, Seconds: 11.35\n",
            "Epoch  11/150 Batch  480/781 - Loss:  1.317, Seconds: 10.95\n",
            "Epoch  11/150 Batch  500/781 - Loss:  1.398, Seconds: 11.21\n",
            "Average loss for this update: 1.396\n",
            "No Improvement.\n",
            "Epoch  11/150 Batch  520/781 - Loss:  1.327, Seconds: 11.15\n",
            "Epoch  11/150 Batch  540/781 - Loss:  1.287, Seconds: 11.61\n",
            "Epoch  11/150 Batch  560/781 - Loss:  1.375, Seconds: 10.88\n",
            "Epoch  11/150 Batch  580/781 - Loss:  1.414, Seconds: 12.48\n",
            "Epoch  11/150 Batch  600/781 - Loss:  1.323, Seconds: 11.05\n",
            "Epoch  11/150 Batch  620/781 - Loss:  1.399, Seconds: 11.55\n",
            "Epoch  11/150 Batch  640/781 - Loss:  1.360, Seconds: 11.67\n",
            "Epoch  11/150 Batch  660/781 - Loss:  1.352, Seconds: 11.66\n",
            "Epoch  11/150 Batch  680/781 - Loss:  1.297, Seconds: 10.55\n",
            "Epoch  11/150 Batch  700/781 - Loss:  1.489, Seconds: 11.46\n",
            "Epoch  11/150 Batch  720/781 - Loss:  1.394, Seconds: 12.02\n",
            "Epoch  11/150 Batch  740/781 - Loss:  1.298, Seconds: 11.52\n",
            "Epoch  11/150 Batch  760/781 - Loss:  1.463, Seconds: 11.12\n",
            "Average loss for this update: 1.372\n",
            "New Record!\n",
            "Epoch  11/150 Batch  780/781 - Loss:  1.365, Seconds: 11.96\n",
            "Epoch  12/150 Batch   20/781 - Loss:  1.454, Seconds: 9.87\n",
            "Epoch  12/150 Batch   40/781 - Loss:  1.294, Seconds: 10.08\n",
            "Epoch  12/150 Batch   60/781 - Loss:  1.226, Seconds: 10.43\n",
            "Epoch  12/150 Batch   80/781 - Loss:  1.379, Seconds: 9.57\n",
            "Epoch  12/150 Batch  100/781 - Loss:  1.370, Seconds: 10.40\n",
            "Epoch  12/150 Batch  120/781 - Loss:  1.292, Seconds: 13.04\n",
            "Epoch  12/150 Batch  140/781 - Loss:  1.327, Seconds: 10.25\n",
            "Epoch  12/150 Batch  160/781 - Loss:  1.330, Seconds: 10.46\n",
            "Epoch  12/150 Batch  180/781 - Loss:  1.320, Seconds: 10.25\n",
            "Epoch  12/150 Batch  200/781 - Loss:  1.214, Seconds: 10.73\n",
            "Epoch  12/150 Batch  220/781 - Loss:  1.303, Seconds: 10.98\n",
            "Epoch  12/150 Batch  240/781 - Loss:  1.496, Seconds: 10.11\n",
            "Average loss for this update: 1.337\n",
            "New Record!\n",
            "Epoch  12/150 Batch  260/781 - Loss:  1.374, Seconds: 12.85\n",
            "Epoch  12/150 Batch  280/781 - Loss:  1.350, Seconds: 11.21\n",
            "Epoch  12/150 Batch  300/781 - Loss:  1.462, Seconds: 11.33\n",
            "Epoch  12/150 Batch  320/781 - Loss:  1.368, Seconds: 10.49\n",
            "Epoch  12/150 Batch  340/781 - Loss:  1.251, Seconds: 10.55\n",
            "Epoch  12/150 Batch  360/781 - Loss:  1.372, Seconds: 10.50\n",
            "Epoch  12/150 Batch  380/781 - Loss:  1.413, Seconds: 10.70\n",
            "Epoch  12/150 Batch  400/781 - Loss:  1.308, Seconds: 10.90\n",
            "Epoch  12/150 Batch  420/781 - Loss:  1.286, Seconds: 11.23\n",
            "Epoch  12/150 Batch  440/781 - Loss:  1.416, Seconds: 10.91\n",
            "Epoch  12/150 Batch  460/781 - Loss:  1.333, Seconds: 11.49\n",
            "Epoch  12/150 Batch  480/781 - Loss:  1.269, Seconds: 11.20\n",
            "Epoch  12/150 Batch  500/781 - Loss:  1.350, Seconds: 11.37\n",
            "Average loss for this update: 1.343\n",
            "No Improvement.\n",
            "Epoch  12/150 Batch  520/781 - Loss:  1.275, Seconds: 11.26\n",
            "Epoch  12/150 Batch  540/781 - Loss:  1.234, Seconds: 11.87\n",
            "Epoch  12/150 Batch  560/781 - Loss:  1.334, Seconds: 10.80\n",
            "Epoch  12/150 Batch  580/781 - Loss:  1.366, Seconds: 10.98\n",
            "Epoch  12/150 Batch  600/781 - Loss:  1.277, Seconds: 11.31\n",
            "Epoch  12/150 Batch  620/781 - Loss:  1.354, Seconds: 11.53\n",
            "Epoch  12/150 Batch  640/781 - Loss:  1.307, Seconds: 11.66\n",
            "Epoch  12/150 Batch  660/781 - Loss:  1.301, Seconds: 11.63\n",
            "Epoch  12/150 Batch  680/781 - Loss:  1.261, Seconds: 10.46\n",
            "Epoch  12/150 Batch  700/781 - Loss:  1.442, Seconds: 11.37\n",
            "Epoch  12/150 Batch  720/781 - Loss:  1.352, Seconds: 11.79\n",
            "Epoch  12/150 Batch  740/781 - Loss:  1.254, Seconds: 11.85\n",
            "Epoch  12/150 Batch  760/781 - Loss:  1.416, Seconds: 11.17\n",
            "Average loss for this update: 1.326\n",
            "New Record!\n",
            "Epoch  12/150 Batch  780/781 - Loss:  1.321, Seconds: 11.47\n",
            "Epoch  13/150 Batch   20/781 - Loss:  1.406, Seconds: 9.92\n",
            "Epoch  13/150 Batch   40/781 - Loss:  1.257, Seconds: 9.91\n",
            "Epoch  13/150 Batch   60/781 - Loss:  1.170, Seconds: 10.30\n",
            "Epoch  13/150 Batch   80/781 - Loss:  1.306, Seconds: 9.66\n",
            "Epoch  13/150 Batch  100/781 - Loss:  1.312, Seconds: 10.44\n",
            "Epoch  13/150 Batch  120/781 - Loss:  1.245, Seconds: 10.94\n",
            "Epoch  13/150 Batch  140/781 - Loss:  1.283, Seconds: 10.21\n",
            "Epoch  13/150 Batch  160/781 - Loss:  1.287, Seconds: 10.57\n",
            "Epoch  13/150 Batch  180/781 - Loss:  1.275, Seconds: 10.32\n",
            "Epoch  13/150 Batch  200/781 - Loss:  1.160, Seconds: 10.64\n",
            "Epoch  13/150 Batch  220/781 - Loss:  1.260, Seconds: 11.01\n",
            "Epoch  13/150 Batch  240/781 - Loss:  1.455, Seconds: 10.13\n",
            "Average loss for this update: 1.288\n",
            "New Record!\n",
            "Epoch  13/150 Batch  260/781 - Loss:  1.325, Seconds: 13.20\n",
            "Epoch  13/150 Batch  280/781 - Loss:  1.304, Seconds: 11.97\n",
            "Epoch  13/150 Batch  300/781 - Loss:  1.428, Seconds: 11.33\n",
            "Epoch  13/150 Batch  320/781 - Loss:  1.333, Seconds: 10.50\n",
            "Epoch  13/150 Batch  340/781 - Loss:  1.222, Seconds: 10.27\n",
            "Epoch  13/150 Batch  360/781 - Loss:  1.323, Seconds: 10.72\n",
            "Epoch  13/150 Batch  380/781 - Loss:  1.368, Seconds: 10.73\n",
            "Epoch  13/150 Batch  400/781 - Loss:  1.263, Seconds: 10.96\n",
            "Epoch  13/150 Batch  420/781 - Loss:  1.234, Seconds: 11.12\n",
            "Epoch  13/150 Batch  440/781 - Loss:  1.341, Seconds: 10.86\n",
            "Epoch  13/150 Batch  460/781 - Loss:  1.284, Seconds: 11.57\n",
            "Epoch  13/150 Batch  480/781 - Loss:  1.217, Seconds: 11.34\n",
            "Epoch  13/150 Batch  500/781 - Loss:  1.295, Seconds: 11.38\n",
            "Average loss for this update: 1.295\n",
            "No Improvement.\n",
            "Epoch  13/150 Batch  520/781 - Loss:  1.226, Seconds: 11.35\n",
            "Epoch  13/150 Batch  540/781 - Loss:  1.181, Seconds: 11.90\n",
            "Epoch  13/150 Batch  560/781 - Loss:  1.299, Seconds: 11.22\n",
            "Epoch  13/150 Batch  580/781 - Loss:  1.336, Seconds: 11.16\n",
            "Epoch  13/150 Batch  600/781 - Loss:  1.231, Seconds: 11.01\n",
            "Epoch  13/150 Batch  620/781 - Loss:  1.293, Seconds: 11.54\n",
            "Epoch  13/150 Batch  640/781 - Loss:  1.265, Seconds: 11.79\n",
            "Epoch  13/150 Batch  660/781 - Loss:  1.251, Seconds: 11.97\n",
            "Epoch  13/150 Batch  680/781 - Loss:  1.204, Seconds: 11.00\n",
            "Epoch  13/150 Batch  700/781 - Loss:  1.391, Seconds: 11.59\n",
            "Epoch  13/150 Batch  720/781 - Loss:  1.303, Seconds: 11.64\n",
            "Epoch  13/150 Batch  740/781 - Loss:  1.204, Seconds: 11.54\n",
            "Epoch  13/150 Batch  760/781 - Loss:  1.359, Seconds: 11.27\n",
            "Average loss for this update: 1.278\n",
            "New Record!\n",
            "Epoch  13/150 Batch  780/781 - Loss:  1.279, Seconds: 11.88\n",
            "Epoch  14/150 Batch   20/781 - Loss:  1.356, Seconds: 10.25\n",
            "Epoch  14/150 Batch   40/781 - Loss:  1.211, Seconds: 10.01\n",
            "Epoch  14/150 Batch   60/781 - Loss:  1.127, Seconds: 10.35\n",
            "Epoch  14/150 Batch   80/781 - Loss:  1.269, Seconds: 9.69\n",
            "Epoch  14/150 Batch  100/781 - Loss:  1.268, Seconds: 10.23\n",
            "Epoch  14/150 Batch  120/781 - Loss:  1.202, Seconds: 10.84\n",
            "Epoch  14/150 Batch  140/781 - Loss:  1.226, Seconds: 10.15\n",
            "Epoch  14/150 Batch  160/781 - Loss:  1.238, Seconds: 10.65\n",
            "Epoch  14/150 Batch  180/781 - Loss:  1.234, Seconds: 10.42\n",
            "Epoch  14/150 Batch  200/781 - Loss:  1.119, Seconds: 10.54\n",
            "Epoch  14/150 Batch  220/781 - Loss:  1.200, Seconds: 11.07\n",
            "Epoch  14/150 Batch  240/781 - Loss:  1.401, Seconds: 10.60\n",
            "Average loss for this update: 1.24\n",
            "New Record!\n",
            "Epoch  14/150 Batch  260/781 - Loss:  1.272, Seconds: 12.47\n",
            "Epoch  14/150 Batch  280/781 - Loss:  1.242, Seconds: 11.01\n",
            "Epoch  14/150 Batch  300/781 - Loss:  1.372, Seconds: 11.07\n",
            "Epoch  14/150 Batch  320/781 - Loss:  1.262, Seconds: 10.59\n",
            "Epoch  14/150 Batch  340/781 - Loss:  1.160, Seconds: 10.43\n",
            "Epoch  14/150 Batch  360/781 - Loss:  1.273, Seconds: 10.74\n",
            "Epoch  14/150 Batch  380/781 - Loss:  1.322, Seconds: 10.65\n",
            "Epoch  14/150 Batch  400/781 - Loss:  1.225, Seconds: 10.69\n",
            "Epoch  14/150 Batch  420/781 - Loss:  1.187, Seconds: 11.25\n",
            "Epoch  14/150 Batch  440/781 - Loss:  1.306, Seconds: 11.31\n",
            "Epoch  14/150 Batch  460/781 - Loss:  1.249, Seconds: 11.55\n",
            "Epoch  14/150 Batch  480/781 - Loss:  1.175, Seconds: 10.86\n",
            "Epoch  14/150 Batch  500/781 - Loss:  1.252, Seconds: 11.27\n",
            "Average loss for this update: 1.247\n",
            "No Improvement.\n",
            "Epoch  14/150 Batch  520/781 - Loss:  1.190, Seconds: 11.24\n",
            "Epoch  14/150 Batch  540/781 - Loss:  1.142, Seconds: 11.72\n",
            "Epoch  14/150 Batch  560/781 - Loss:  1.242, Seconds: 11.35\n",
            "Epoch  14/150 Batch  580/781 - Loss:  1.290, Seconds: 11.23\n",
            "Epoch  14/150 Batch  600/781 - Loss:  1.197, Seconds: 11.36\n",
            "Epoch  14/150 Batch  620/781 - Loss:  1.255, Seconds: 11.87\n",
            "Epoch  14/150 Batch  640/781 - Loss:  1.229, Seconds: 11.56\n",
            "Epoch  14/150 Batch  660/781 - Loss:  1.218, Seconds: 11.49\n",
            "Epoch  14/150 Batch  680/781 - Loss:  1.164, Seconds: 10.62\n",
            "Epoch  14/150 Batch  700/781 - Loss:  1.349, Seconds: 10.99\n",
            "Epoch  14/150 Batch  720/781 - Loss:  1.262, Seconds: 11.51\n",
            "Epoch  14/150 Batch  740/781 - Loss:  1.170, Seconds: 11.78\n",
            "Epoch  14/150 Batch  760/781 - Loss:  1.343, Seconds: 11.23\n",
            "Average loss for this update: 1.241\n",
            "No Improvement.\n",
            "Epoch  14/150 Batch  780/781 - Loss:  1.252, Seconds: 11.65\n",
            "Epoch  15/150 Batch   20/781 - Loss:  1.317, Seconds: 10.05\n",
            "Epoch  15/150 Batch   40/781 - Loss:  1.174, Seconds: 10.10\n",
            "Epoch  15/150 Batch   60/781 - Loss:  1.094, Seconds: 10.30\n",
            "Epoch  15/150 Batch   80/781 - Loss:  1.231, Seconds: 9.80\n",
            "Epoch  15/150 Batch  100/781 - Loss:  1.246, Seconds: 10.31\n",
            "Epoch  15/150 Batch  120/781 - Loss:  1.167, Seconds: 11.00\n",
            "Epoch  15/150 Batch  140/781 - Loss:  1.203, Seconds: 10.29\n",
            "Epoch  15/150 Batch  160/781 - Loss:  1.206, Seconds: 10.24\n",
            "Epoch  15/150 Batch  180/781 - Loss:  1.216, Seconds: 10.47\n",
            "Epoch  15/150 Batch  200/781 - Loss:  1.089, Seconds: 10.49\n",
            "Epoch  15/150 Batch  220/781 - Loss:  1.169, Seconds: 11.37\n",
            "Epoch  15/150 Batch  240/781 - Loss:  1.356, Seconds: 10.20\n",
            "Average loss for this update: 1.207\n",
            "New Record!\n",
            "Epoch  15/150 Batch  260/781 - Loss:  1.227, Seconds: 13.03\n",
            "Epoch  15/150 Batch  280/781 - Loss:  1.217, Seconds: 11.16\n",
            "Epoch  15/150 Batch  300/781 - Loss:  1.329, Seconds: 11.32\n",
            "Epoch  15/150 Batch  320/781 - Loss:  1.238, Seconds: 10.71\n",
            "Epoch  15/150 Batch  340/781 - Loss:  1.118, Seconds: 10.43\n",
            "Epoch  15/150 Batch  360/781 - Loss:  1.228, Seconds: 11.85\n",
            "Epoch  15/150 Batch  380/781 - Loss:  1.269, Seconds: 10.65\n",
            "Epoch  15/150 Batch  400/781 - Loss:  1.189, Seconds: 10.50\n",
            "Epoch  15/150 Batch  420/781 - Loss:  1.150, Seconds: 10.94\n",
            "Epoch  15/150 Batch  440/781 - Loss:  1.260, Seconds: 10.62\n",
            "Epoch  15/150 Batch  460/781 - Loss:  1.206, Seconds: 11.48\n",
            "Epoch  15/150 Batch  480/781 - Loss:  1.133, Seconds: 10.69\n",
            "Epoch  15/150 Batch  500/781 - Loss:  1.216, Seconds: 11.29\n",
            "Average loss for this update: 1.207\n",
            "No Improvement.\n",
            "Epoch  15/150 Batch  520/781 - Loss:  1.143, Seconds: 11.10\n",
            "Epoch  15/150 Batch  540/781 - Loss:  1.099, Seconds: 11.50\n",
            "Epoch  15/150 Batch  560/781 - Loss:  1.201, Seconds: 10.85\n",
            "Epoch  15/150 Batch  580/781 - Loss:  1.250, Seconds: 11.11\n",
            "Epoch  15/150 Batch  600/781 - Loss:  1.158, Seconds: 11.06\n",
            "Epoch  15/150 Batch  620/781 - Loss:  1.215, Seconds: 11.60\n",
            "Epoch  15/150 Batch  640/781 - Loss:  1.188, Seconds: 11.52\n",
            "Epoch  15/150 Batch  660/781 - Loss:  1.165, Seconds: 11.92\n",
            "Epoch  15/150 Batch  680/781 - Loss:  1.128, Seconds: 10.46\n",
            "Epoch  15/150 Batch  700/781 - Loss:  1.288, Seconds: 10.97\n",
            "Epoch  15/150 Batch  720/781 - Loss:  1.219, Seconds: 11.60\n",
            "Epoch  15/150 Batch  740/781 - Loss:  1.139, Seconds: 11.81\n",
            "Epoch  15/150 Batch  760/781 - Loss:  1.300, Seconds: 11.13\n",
            "Average loss for this update: 1.197\n",
            "New Record!\n",
            "Epoch  15/150 Batch  780/781 - Loss:  1.193, Seconds: 11.75\n",
            "Epoch  16/150 Batch   20/781 - Loss:  1.267, Seconds: 10.34\n",
            "Epoch  16/150 Batch   40/781 - Loss:  1.127, Seconds: 10.13\n",
            "Epoch  16/150 Batch   60/781 - Loss:  1.050, Seconds: 10.55\n",
            "Epoch  16/150 Batch   80/781 - Loss:  1.183, Seconds: 9.76\n",
            "Epoch  16/150 Batch  100/781 - Loss:  1.192, Seconds: 10.39\n",
            "Epoch  16/150 Batch  120/781 - Loss:  1.127, Seconds: 10.84\n",
            "Epoch  16/150 Batch  140/781 - Loss:  1.157, Seconds: 10.31\n",
            "Epoch  16/150 Batch  160/781 - Loss:  1.155, Seconds: 10.46\n",
            "Epoch  16/150 Batch  180/781 - Loss:  1.170, Seconds: 10.55\n",
            "Epoch  16/150 Batch  200/781 - Loss:  1.049, Seconds: 10.59\n",
            "Epoch  16/150 Batch  220/781 - Loss:  1.130, Seconds: 11.02\n",
            "Epoch  16/150 Batch  240/781 - Loss:  1.299, Seconds: 9.95\n",
            "Average loss for this update: 1.16\n",
            "New Record!\n",
            "Epoch  16/150 Batch  260/781 - Loss:  1.179, Seconds: 15.41\n",
            "Epoch  16/150 Batch  280/781 - Loss:  1.169, Seconds: 10.62\n",
            "Epoch  16/150 Batch  300/781 - Loss:  1.294, Seconds: 10.87\n",
            "Epoch  16/150 Batch  320/781 - Loss:  1.184, Seconds: 10.37\n",
            "Epoch  16/150 Batch  340/781 - Loss:  1.068, Seconds: 10.13\n",
            "Epoch  16/150 Batch  360/781 - Loss:  1.180, Seconds: 10.55\n",
            "Epoch  16/150 Batch  380/781 - Loss:  1.224, Seconds: 10.69\n",
            "Epoch  16/150 Batch  400/781 - Loss:  1.148, Seconds: 10.49\n",
            "Epoch  16/150 Batch  420/781 - Loss:  1.106, Seconds: 10.94\n",
            "Epoch  16/150 Batch  440/781 - Loss:  1.205, Seconds: 10.79\n",
            "Epoch  16/150 Batch  460/781 - Loss:  1.162, Seconds: 11.53\n",
            "Epoch  16/150 Batch  480/781 - Loss:  1.096, Seconds: 10.93\n",
            "Epoch  16/150 Batch  500/781 - Loss:  1.160, Seconds: 11.26\n",
            "Average loss for this update: 1.162\n",
            "No Improvement.\n",
            "Epoch  16/150 Batch  520/781 - Loss:  1.108, Seconds: 10.96\n",
            "Epoch  16/150 Batch  540/781 - Loss:  1.058, Seconds: 11.81\n",
            "Epoch  16/150 Batch  560/781 - Loss:  1.155, Seconds: 11.07\n",
            "Epoch  16/150 Batch  580/781 - Loss:  1.215, Seconds: 11.22\n",
            "Epoch  16/150 Batch  600/781 - Loss:  1.121, Seconds: 11.39\n",
            "Epoch  16/150 Batch  620/781 - Loss:  1.174, Seconds: 11.86\n",
            "Epoch  16/150 Batch  640/781 - Loss:  1.170, Seconds: 11.72\n",
            "Epoch  16/150 Batch  660/781 - Loss:  1.136, Seconds: 11.75\n",
            "Epoch  16/150 Batch  680/781 - Loss:  1.088, Seconds: 10.45\n",
            "Epoch  16/150 Batch  700/781 - Loss:  1.246, Seconds: 11.05\n",
            "Epoch  16/150 Batch  720/781 - Loss:  1.182, Seconds: 11.59\n",
            "Epoch  16/150 Batch  740/781 - Loss:  1.101, Seconds: 11.63\n",
            "Epoch  16/150 Batch  760/781 - Loss:  1.252, Seconds: 11.22\n",
            "Average loss for this update: 1.16\n",
            "New Record!\n",
            "Epoch  16/150 Batch  780/781 - Loss:  1.162, Seconds: 11.42\n",
            "Epoch  17/150 Batch   20/781 - Loss:  1.220, Seconds: 10.05\n",
            "Epoch  17/150 Batch   40/781 - Loss:  1.088, Seconds: 10.24\n",
            "Epoch  17/150 Batch   60/781 - Loss:  1.010, Seconds: 10.38\n",
            "Epoch  17/150 Batch   80/781 - Loss:  1.139, Seconds: 9.70\n",
            "Epoch  17/150 Batch  100/781 - Loss:  1.151, Seconds: 10.28\n",
            "Epoch  17/150 Batch  120/781 - Loss:  1.092, Seconds: 10.56\n",
            "Epoch  17/150 Batch  140/781 - Loss:  1.116, Seconds: 10.05\n",
            "Epoch  17/150 Batch  160/781 - Loss:  1.113, Seconds: 10.34\n",
            "Epoch  17/150 Batch  180/781 - Loss:  1.131, Seconds: 10.41\n",
            "Epoch  17/150 Batch  200/781 - Loss:  1.019, Seconds: 10.53\n",
            "Epoch  17/150 Batch  220/781 - Loss:  1.091, Seconds: 11.25\n",
            "Epoch  17/150 Batch  240/781 - Loss:  1.258, Seconds: 10.08\n",
            "Average loss for this update: 1.121\n",
            "New Record!\n",
            "Epoch  17/150 Batch  260/781 - Loss:  1.149, Seconds: 11.52\n",
            "Epoch  17/150 Batch  280/781 - Loss:  1.130, Seconds: 10.97\n",
            "Epoch  17/150 Batch  300/781 - Loss:  1.240, Seconds: 10.75\n",
            "Epoch  17/150 Batch  320/781 - Loss:  1.141, Seconds: 10.99\n",
            "Epoch  17/150 Batch  340/781 - Loss:  1.047, Seconds: 10.35\n",
            "Epoch  17/150 Batch  360/781 - Loss:  1.141, Seconds: 10.51\n",
            "Epoch  17/150 Batch  380/781 - Loss:  1.176, Seconds: 10.62\n",
            "Epoch  17/150 Batch  400/781 - Loss:  1.111, Seconds: 10.88\n",
            "Epoch  17/150 Batch  420/781 - Loss:  1.072, Seconds: 11.09\n",
            "Epoch  17/150 Batch  440/781 - Loss:  1.155, Seconds: 11.12\n",
            "Epoch  17/150 Batch  460/781 - Loss:  1.114, Seconds: 11.43\n",
            "Epoch  17/150 Batch  480/781 - Loss:  1.049, Seconds: 10.79\n",
            "Epoch  17/150 Batch  500/781 - Loss:  1.114, Seconds: 11.24\n",
            "Average loss for this update: 1.12\n",
            "New Record!\n",
            "Epoch  17/150 Batch  520/781 - Loss:  1.073, Seconds: 10.92\n",
            "Epoch  17/150 Batch  540/781 - Loss:  1.041, Seconds: 11.61\n",
            "Epoch  17/150 Batch  560/781 - Loss:  1.127, Seconds: 10.67\n",
            "Epoch  17/150 Batch  580/781 - Loss:  1.185, Seconds: 11.12\n",
            "Epoch  17/150 Batch  600/781 - Loss:  1.089, Seconds: 11.33\n",
            "Epoch  17/150 Batch  620/781 - Loss:  1.146, Seconds: 11.61\n",
            "Epoch  17/150 Batch  640/781 - Loss:  1.116, Seconds: 11.92\n",
            "Epoch  17/150 Batch  660/781 - Loss:  1.098, Seconds: 11.78\n",
            "Epoch  17/150 Batch  680/781 - Loss:  1.044, Seconds: 10.63\n",
            "Epoch  17/150 Batch  700/781 - Loss:  1.206, Seconds: 11.13\n",
            "Epoch  17/150 Batch  720/781 - Loss:  1.153, Seconds: 11.69\n",
            "Epoch  17/150 Batch  740/781 - Loss:  1.064, Seconds: 11.58\n",
            "Epoch  17/150 Batch  760/781 - Loss:  1.221, Seconds: 11.12\n",
            "Average loss for this update: 1.126\n",
            "No Improvement.\n",
            "Epoch  17/150 Batch  780/781 - Loss:  1.141, Seconds: 11.86\n",
            "Epoch  18/150 Batch   20/781 - Loss:  1.184, Seconds: 10.97\n",
            "Epoch  18/150 Batch   40/781 - Loss:  1.054, Seconds: 10.20\n",
            "Epoch  18/150 Batch   60/781 - Loss:  0.983, Seconds: 10.11\n",
            "Epoch  18/150 Batch   80/781 - Loss:  1.104, Seconds: 9.52\n",
            "Epoch  18/150 Batch  100/781 - Loss:  1.117, Seconds: 10.31\n",
            "Epoch  18/150 Batch  120/781 - Loss:  1.056, Seconds: 11.27\n",
            "Epoch  18/150 Batch  140/781 - Loss:  1.091, Seconds: 10.26\n",
            "Epoch  18/150 Batch  160/781 - Loss:  1.082, Seconds: 10.28\n",
            "Epoch  18/150 Batch  180/781 - Loss:  1.105, Seconds: 10.64\n",
            "Epoch  18/150 Batch  200/781 - Loss:  0.989, Seconds: 10.56\n",
            "Epoch  18/150 Batch  220/781 - Loss:  1.074, Seconds: 10.98\n",
            "Epoch  18/150 Batch  240/781 - Loss:  1.225, Seconds: 10.16\n",
            "Average loss for this update: 1.09\n",
            "New Record!\n",
            "Epoch  18/150 Batch  260/781 - Loss:  1.108, Seconds: 12.02\n",
            "Epoch  18/150 Batch  280/781 - Loss:  1.095, Seconds: 10.81\n",
            "Epoch  18/150 Batch  300/781 - Loss:  1.209, Seconds: 11.19\n",
            "Epoch  18/150 Batch  320/781 - Loss:  1.113, Seconds: 10.50\n",
            "Epoch  18/150 Batch  340/781 - Loss:  1.009, Seconds: 10.52\n",
            "Epoch  18/150 Batch  360/781 - Loss:  1.096, Seconds: 10.68\n",
            "Epoch  18/150 Batch  380/781 - Loss:  1.155, Seconds: 10.75\n",
            "Epoch  18/150 Batch  400/781 - Loss:  1.074, Seconds: 11.06\n",
            "Epoch  18/150 Batch  420/781 - Loss:  1.031, Seconds: 10.90\n",
            "Epoch  18/150 Batch  440/781 - Loss:  1.138, Seconds: 10.83\n",
            "Epoch  18/150 Batch  460/781 - Loss:  1.093, Seconds: 11.55\n",
            "Epoch  18/150 Batch  480/781 - Loss:  1.036, Seconds: 11.05\n",
            "Epoch  18/150 Batch  500/781 - Loss:  1.103, Seconds: 11.39\n",
            "Average loss for this update: 1.091\n",
            "No Improvement.\n",
            "Epoch  18/150 Batch  520/781 - Loss:  1.040, Seconds: 11.04\n",
            "Epoch  18/150 Batch  540/781 - Loss:  1.004, Seconds: 11.65\n",
            "Epoch  18/150 Batch  560/781 - Loss:  1.089, Seconds: 11.02\n",
            "Epoch  18/150 Batch  580/781 - Loss:  1.142, Seconds: 11.46\n",
            "Epoch  18/150 Batch  600/781 - Loss:  1.046, Seconds: 11.27\n",
            "Epoch  18/150 Batch  620/781 - Loss:  1.113, Seconds: 11.93\n",
            "Epoch  18/150 Batch  640/781 - Loss:  1.087, Seconds: 11.45\n",
            "Epoch  18/150 Batch  660/781 - Loss:  1.075, Seconds: 11.58\n",
            "Epoch  18/150 Batch  680/781 - Loss:  1.022, Seconds: 10.18\n",
            "Epoch  18/150 Batch  700/781 - Loss:  1.175, Seconds: 11.05\n",
            "Epoch  18/150 Batch  720/781 - Loss:  1.113, Seconds: 11.64\n",
            "Epoch  18/150 Batch  740/781 - Loss:  1.038, Seconds: 11.59\n",
            "Epoch  18/150 Batch  760/781 - Loss:  1.195, Seconds: 11.20\n",
            "Average loss for this update: 1.094\n",
            "No Improvement.\n",
            "Epoch  18/150 Batch  780/781 - Loss:  1.115, Seconds: 11.59\n",
            "Epoch  19/150 Batch   20/781 - Loss:  1.150, Seconds: 10.00\n",
            "Epoch  19/150 Batch   40/781 - Loss:  1.023, Seconds: 10.17\n",
            "Epoch  19/150 Batch   60/781 - Loss:  0.953, Seconds: 10.68\n",
            "Epoch  19/150 Batch   80/781 - Loss:  1.063, Seconds: 9.68\n",
            "Epoch  19/150 Batch  100/781 - Loss:  1.090, Seconds: 10.14\n",
            "Epoch  19/150 Batch  120/781 - Loss:  1.027, Seconds: 10.91\n",
            "Epoch  19/150 Batch  140/781 - Loss:  1.052, Seconds: 10.39\n",
            "Epoch  19/150 Batch  160/781 - Loss:  1.090, Seconds: 10.19\n",
            "Epoch  19/150 Batch  180/781 - Loss:  1.098, Seconds: 10.29\n",
            "Epoch  19/150 Batch  200/781 - Loss:  1.029, Seconds: 10.56\n",
            "Epoch  19/150 Batch  220/781 - Loss:  1.094, Seconds: 10.89\n",
            "Epoch  19/150 Batch  240/781 - Loss:  1.225, Seconds: 10.23\n",
            "Average loss for this update: 1.08\n",
            "New Record!\n",
            "Epoch  19/150 Batch  260/781 - Loss:  1.138, Seconds: 13.14\n",
            "Epoch  19/150 Batch  280/781 - Loss:  1.098, Seconds: 11.49\n",
            "Epoch  19/150 Batch  300/781 - Loss:  1.194, Seconds: 11.02\n",
            "Epoch  19/150 Batch  320/781 - Loss:  1.106, Seconds: 10.54\n",
            "Epoch  19/150 Batch  340/781 - Loss:  1.005, Seconds: 10.22\n",
            "Epoch  19/150 Batch  360/781 - Loss:  2.075, Seconds: 10.65\n",
            "Epoch  19/150 Batch  380/781 - Loss:  1.391, Seconds: 10.90\n",
            "Epoch  19/150 Batch  400/781 - Loss:  1.233, Seconds: 10.97\n",
            "Epoch  19/150 Batch  420/781 - Loss:  1.168, Seconds: 10.95\n",
            "Epoch  19/150 Batch  440/781 - Loss:  1.274, Seconds: 10.93\n",
            "Epoch  19/150 Batch  460/781 - Loss:  1.203, Seconds: 11.76\n",
            "Epoch  19/150 Batch  480/781 - Loss:  1.117, Seconds: 10.67\n",
            "Epoch  19/150 Batch  500/781 - Loss:  1.196, Seconds: 11.38\n",
            "Average loss for this update: 1.245\n",
            "No Improvement.\n",
            "Epoch  19/150 Batch  520/781 - Loss:  1.136, Seconds: 11.58\n",
            "Epoch  19/150 Batch  540/781 - Loss:  1.078, Seconds: 11.41\n",
            "Epoch  19/150 Batch  560/781 - Loss:  1.151, Seconds: 11.08\n",
            "Epoch  19/150 Batch  580/781 - Loss:  1.238, Seconds: 10.95\n",
            "Epoch  19/150 Batch  600/781 - Loss:  1.132, Seconds: 11.04\n",
            "Epoch  19/150 Batch  620/781 - Loss:  1.174, Seconds: 11.63\n",
            "Epoch  19/150 Batch  640/781 - Loss:  1.158, Seconds: 11.57\n",
            "Epoch  19/150 Batch  660/781 - Loss:  1.129, Seconds: 11.39\n",
            "Epoch  19/150 Batch  680/781 - Loss:  1.072, Seconds: 10.10\n",
            "Epoch  19/150 Batch  700/781 - Loss:  1.222, Seconds: 10.93\n",
            "Epoch  19/150 Batch  720/781 - Loss:  1.151, Seconds: 11.51\n",
            "Epoch  19/150 Batch  740/781 - Loss:  1.084, Seconds: 11.73\n",
            "Epoch  19/150 Batch  760/781 - Loss:  1.215, Seconds: 10.86\n",
            "Average loss for this update: 1.15\n",
            "No Improvement.\n",
            "Epoch  19/150 Batch  780/781 - Loss:  1.130, Seconds: 11.50\n",
            "Epoch  20/150 Batch   20/781 - Loss:  1.179, Seconds: 10.40\n",
            "Epoch  20/150 Batch   40/781 - Loss:  1.052, Seconds: 10.23\n",
            "Epoch  20/150 Batch   60/781 - Loss:  0.983, Seconds: 10.15\n",
            "Epoch  20/150 Batch   80/781 - Loss:  1.088, Seconds: 9.56\n",
            "Epoch  20/150 Batch  100/781 - Loss:  1.107, Seconds: 10.17\n",
            "Epoch  20/150 Batch  120/781 - Loss:  1.048, Seconds: 10.80\n",
            "Epoch  20/150 Batch  140/781 - Loss:  1.068, Seconds: 10.48\n",
            "Epoch  20/150 Batch  160/781 - Loss:  1.066, Seconds: 10.61\n",
            "Epoch  20/150 Batch  180/781 - Loss:  1.077, Seconds: 10.24\n",
            "Epoch  20/150 Batch  200/781 - Loss:  0.970, Seconds: 10.60\n",
            "Epoch  20/150 Batch  220/781 - Loss:  1.056, Seconds: 11.18\n",
            "Epoch  20/150 Batch  240/781 - Loss:  1.214, Seconds: 10.22\n",
            "Average loss for this update: 1.078\n",
            "New Record!\n",
            "Epoch  20/150 Batch  260/781 - Loss:  1.111, Seconds: 11.46\n",
            "Epoch  20/150 Batch  280/781 - Loss:  1.080, Seconds: 11.16\n",
            "Epoch  20/150 Batch  300/781 - Loss:  1.174, Seconds: 10.76\n",
            "Epoch  20/150 Batch  320/781 - Loss:  1.084, Seconds: 10.61\n",
            "Epoch  20/150 Batch  340/781 - Loss:  0.990, Seconds: 10.33\n",
            "Epoch  20/150 Batch  360/781 - Loss:  1.080, Seconds: 10.59\n",
            "Epoch  20/150 Batch  380/781 - Loss:  1.151, Seconds: 10.82\n",
            "Epoch  20/150 Batch  400/781 - Loss:  1.061, Seconds: 10.78\n",
            "Epoch  20/150 Batch  420/781 - Loss:  1.021, Seconds: 11.06\n",
            "Epoch  20/150 Batch  440/781 - Loss:  1.097, Seconds: 10.96\n",
            "Epoch  20/150 Batch  460/781 - Loss:  1.074, Seconds: 11.44\n",
            "Epoch  20/150 Batch  480/781 - Loss:  1.009, Seconds: 10.87\n",
            "Epoch  20/150 Batch  500/781 - Loss:  1.061, Seconds: 11.31\n",
            "Average loss for this update: 1.069\n",
            "New Record!\n",
            "Epoch  20/150 Batch  520/781 - Loss:  1.009, Seconds: 11.18\n",
            "Epoch  20/150 Batch  540/781 - Loss:  0.985, Seconds: 11.45\n",
            "Epoch  20/150 Batch  560/781 - Loss:  1.054, Seconds: 11.07\n",
            "Epoch  20/150 Batch  580/781 - Loss:  1.108, Seconds: 11.28\n",
            "Epoch  20/150 Batch  600/781 - Loss:  1.031, Seconds: 11.22\n",
            "Epoch  20/150 Batch  620/781 - Loss:  1.081, Seconds: 11.44\n",
            "Epoch  20/150 Batch  640/781 - Loss:  1.055, Seconds: 11.54\n",
            "Epoch  20/150 Batch  660/781 - Loss:  1.047, Seconds: 11.40\n",
            "Epoch  20/150 Batch  680/781 - Loss:  0.992, Seconds: 10.61\n",
            "Epoch  20/150 Batch  700/781 - Loss:  1.113, Seconds: 11.00\n",
            "Epoch  20/150 Batch  720/781 - Loss:  1.067, Seconds: 11.84\n",
            "Epoch  20/150 Batch  740/781 - Loss:  0.998, Seconds: 11.82\n",
            "Epoch  20/150 Batch  760/781 - Loss:  1.130, Seconds: 11.16\n",
            "Average loss for this update: 1.057\n",
            "New Record!\n",
            "Epoch  20/150 Batch  780/781 - Loss:  1.070, Seconds: 11.43\n",
            "Epoch  21/150 Batch   20/781 - Loss:  1.092, Seconds: 10.92\n",
            "Epoch  21/150 Batch   40/781 - Loss:  0.975, Seconds: 9.86\n",
            "Epoch  21/150 Batch   60/781 - Loss:  0.913, Seconds: 10.57\n",
            "Epoch  21/150 Batch   80/781 - Loss:  1.009, Seconds: 9.58\n",
            "Epoch  21/150 Batch  100/781 - Loss:  1.023, Seconds: 10.08\n",
            "Epoch  21/150 Batch  120/781 - Loss:  0.981, Seconds: 10.70\n",
            "Epoch  21/150 Batch  140/781 - Loss:  1.021, Seconds: 10.14\n",
            "Epoch  21/150 Batch  160/781 - Loss:  1.013, Seconds: 10.37\n",
            "Epoch  21/150 Batch  180/781 - Loss:  1.021, Seconds: 10.69\n",
            "Epoch  21/150 Batch  200/781 - Loss:  0.920, Seconds: 10.55\n",
            "Epoch  21/150 Batch  220/781 - Loss:  1.285, Seconds: 11.04\n",
            "Epoch  21/150 Batch  240/781 - Loss:  1.394, Seconds: 10.47\n",
            "Average loss for this update: 1.068\n",
            "No Improvement.\n",
            "Epoch  21/150 Batch  260/781 - Loss:  1.242, Seconds: 10.45\n",
            "Epoch  21/150 Batch  280/781 - Loss:  1.180, Seconds: 11.08\n",
            "Epoch  21/150 Batch  300/781 - Loss:  1.248, Seconds: 10.93\n",
            "Epoch  21/150 Batch  320/781 - Loss:  1.159, Seconds: 11.02\n",
            "Epoch  21/150 Batch  340/781 - Loss:  1.063, Seconds: 10.20\n",
            "Epoch  21/150 Batch  360/781 - Loss:  1.148, Seconds: 10.43\n",
            "Epoch  21/150 Batch  380/781 - Loss:  1.196, Seconds: 10.57\n",
            "Epoch  21/150 Batch  400/781 - Loss:  1.115, Seconds: 10.80\n",
            "Epoch  21/150 Batch  420/781 - Loss:  1.078, Seconds: 11.21\n",
            "Epoch  21/150 Batch  440/781 - Loss:  1.140, Seconds: 11.16\n",
            "Epoch  21/150 Batch  460/781 - Loss:  1.104, Seconds: 11.51\n",
            "Epoch  21/150 Batch  480/781 - Loss:  1.043, Seconds: 10.86\n",
            "Epoch  21/150 Batch  500/781 - Loss:  1.102, Seconds: 11.60\n",
            "Average loss for this update: 1.125\n",
            "No Improvement.\n",
            "Epoch  21/150 Batch  520/781 - Loss:  1.046, Seconds: 11.36\n",
            "Epoch  21/150 Batch  540/781 - Loss:  1.016, Seconds: 11.89\n",
            "Epoch  21/150 Batch  560/781 - Loss:  1.102, Seconds: 10.61\n",
            "Epoch  21/150 Batch  580/781 - Loss:  1.149, Seconds: 11.11\n",
            "Epoch  21/150 Batch  600/781 - Loss:  1.074, Seconds: 11.10\n",
            "Epoch  21/150 Batch  620/781 - Loss:  1.129, Seconds: 11.47\n",
            "Epoch  21/150 Batch  640/781 - Loss:  1.102, Seconds: 11.48\n",
            "Epoch  21/150 Batch  660/781 - Loss:  1.078, Seconds: 11.34\n",
            "Epoch  21/150 Batch  680/781 - Loss:  1.030, Seconds: 10.48\n",
            "Epoch  21/150 Batch  700/781 - Loss:  1.176, Seconds: 11.38\n",
            "Epoch  21/150 Batch  720/781 - Loss:  1.105, Seconds: 11.87\n",
            "Epoch  21/150 Batch  740/781 - Loss:  1.045, Seconds: 11.87\n",
            "Epoch  21/150 Batch  760/781 - Loss:  1.173, Seconds: 11.28\n",
            "Average loss for this update: 1.099\n",
            "No Improvement.\n",
            "Epoch  21/150 Batch  780/781 - Loss:  1.099, Seconds: 11.41\n",
            "Epoch  22/150 Batch   20/781 - Loss:  1.136, Seconds: 10.19\n",
            "Epoch  22/150 Batch   40/781 - Loss:  1.010, Seconds: 10.04\n",
            "Epoch  22/150 Batch   60/781 - Loss:  0.954, Seconds: 10.10\n",
            "Epoch  22/150 Batch   80/781 - Loss:  1.041, Seconds: 9.69\n",
            "Epoch  22/150 Batch  100/781 - Loss:  1.062, Seconds: 10.26\n",
            "Epoch  22/150 Batch  120/781 - Loss:  1.008, Seconds: 10.70\n",
            "Epoch  22/150 Batch  140/781 - Loss:  1.231, Seconds: 10.62\n",
            "Epoch  22/150 Batch  160/781 - Loss:  2.224, Seconds: 10.46\n",
            "Epoch  22/150 Batch  180/781 - Loss:  1.713, Seconds: 10.52\n",
            "Epoch  22/150 Batch  200/781 - Loss:  1.465, Seconds: 10.55\n",
            "Epoch  22/150 Batch  220/781 - Loss:  2.543, Seconds: 10.88\n",
            "Epoch  22/150 Batch  240/781 - Loss:  1.675, Seconds: 10.28\n",
            "Average loss for this update: 1.43\n",
            "No Improvement.\n",
            "Epoch  22/150 Batch  260/781 - Loss:  1.530, Seconds: 10.70\n",
            "Epoch  22/150 Batch  280/781 - Loss:  1.485, Seconds: 11.01\n",
            "Epoch  22/150 Batch  300/781 - Loss:  1.593, Seconds: 10.78\n",
            "Epoch  22/150 Batch  320/781 - Loss:  1.458, Seconds: 10.25\n",
            "Epoch  22/150 Batch  340/781 - Loss:  1.365, Seconds: 10.36\n",
            "Epoch  22/150 Batch  360/781 - Loss:  1.447, Seconds: 10.62\n",
            "Epoch  22/150 Batch  380/781 - Loss:  1.507, Seconds: 10.86\n",
            "Epoch  22/150 Batch  400/781 - Loss:  1.421, Seconds: 11.07\n",
            "Epoch  22/150 Batch  420/781 - Loss:  1.359, Seconds: 10.88\n",
            "Epoch  22/150 Batch  440/781 - Loss:  1.488, Seconds: 10.95\n",
            "Epoch  22/150 Batch  460/781 - Loss:  1.405, Seconds: 11.78\n",
            "Epoch  22/150 Batch  480/781 - Loss:  1.328, Seconds: 10.79\n",
            "Epoch  22/150 Batch  500/781 - Loss:  1.399, Seconds: 11.45\n",
            "Average loss for this update: 1.431\n",
            "No Improvement.\n",
            "Epoch  22/150 Batch  520/781 - Loss:  1.344, Seconds: 11.08\n",
            "Epoch  22/150 Batch  540/781 - Loss:  1.305, Seconds: 11.59\n",
            "Epoch  22/150 Batch  560/781 - Loss:  1.355, Seconds: 11.08\n",
            "Epoch  22/150 Batch  580/781 - Loss:  1.398, Seconds: 11.27\n",
            "Epoch  22/150 Batch  600/781 - Loss:  1.330, Seconds: 11.45\n",
            "Epoch  22/150 Batch  620/781 - Loss:  1.354, Seconds: 11.60\n",
            "Epoch  22/150 Batch  640/781 - Loss:  1.371, Seconds: 11.63\n",
            "Epoch  22/150 Batch  660/781 - Loss:  1.348, Seconds: 11.49\n",
            "Epoch  22/150 Batch  680/781 - Loss:  1.272, Seconds: 10.33\n",
            "Epoch  22/150 Batch  700/781 - Loss:  1.413, Seconds: 11.14\n",
            "Epoch  22/150 Batch  720/781 - Loss:  1.347, Seconds: 11.36\n",
            "Epoch  22/150 Batch  740/781 - Loss:  1.267, Seconds: 11.55\n",
            "Epoch  22/150 Batch  760/781 - Loss:  1.392, Seconds: 10.79\n",
            "Average loss for this update: 1.344\n",
            "No Improvement.\n",
            "Epoch  22/150 Batch  780/781 - Loss:  1.296, Seconds: 11.45\n",
            "Epoch  23/150 Batch   20/781 - Loss:  1.372, Seconds: 10.01\n",
            "Epoch  23/150 Batch   40/781 - Loss:  1.227, Seconds: 9.91\n",
            "Epoch  23/150 Batch   60/781 - Loss:  1.153, Seconds: 10.04\n",
            "Epoch  23/150 Batch   80/781 - Loss:  1.272, Seconds: 9.84\n",
            "Epoch  23/150 Batch  100/781 - Loss:  1.296, Seconds: 10.18\n",
            "Epoch  23/150 Batch  120/781 - Loss:  1.204, Seconds: 10.81\n",
            "Epoch  23/150 Batch  140/781 - Loss:  1.253, Seconds: 9.95\n",
            "Epoch  23/150 Batch  160/781 - Loss:  2.845, Seconds: 10.19\n",
            "Epoch  23/150 Batch  180/781 - Loss:  1.242, Seconds: 10.11\n",
            "Epoch  23/150 Batch  200/781 - Loss:  1.140, Seconds: 10.57\n",
            "Epoch  23/150 Batch  220/781 - Loss:  1.965, Seconds: 11.29\n",
            "Epoch  23/150 Batch  240/781 - Loss:  1.375, Seconds: 10.21\n",
            "Average loss for this update: 1.433\n",
            "No Improvement.\n",
            "Epoch  23/150 Batch  260/781 - Loss:  1.283, Seconds: 10.69\n",
            "Epoch  23/150 Batch  280/781 - Loss:  1.268, Seconds: 10.85\n",
            "Epoch  23/150 Batch  300/781 - Loss:  1.329, Seconds: 10.84\n",
            "Epoch  23/150 Batch  320/781 - Loss:  1.235, Seconds: 10.57\n",
            "Epoch  23/150 Batch  340/781 - Loss:  1.151, Seconds: 10.34\n",
            "Epoch  23/150 Batch  360/781 - Loss:  1.234, Seconds: 10.73\n",
            "Epoch  23/150 Batch  380/781 - Loss:  1.268, Seconds: 10.86\n",
            "Epoch  23/150 Batch  400/781 - Loss:  1.207, Seconds: 10.89\n",
            "Epoch  23/150 Batch  420/781 - Loss:  1.184, Seconds: 10.89\n",
            "Epoch  23/150 Batch  440/781 - Loss:  1.253, Seconds: 11.00\n",
            "Epoch  23/150 Batch  460/781 - Loss:  1.213, Seconds: 11.39\n",
            "Epoch  23/150 Batch  480/781 - Loss:  1.153, Seconds: 10.79\n",
            "Epoch  23/150 Batch  500/781 - Loss:  1.206, Seconds: 11.17\n",
            "Average loss for this update: 1.22\n",
            "No Improvement.\n",
            "Epoch  23/150 Batch  520/781 - Loss:  1.149, Seconds: 11.30\n",
            "Epoch  23/150 Batch  540/781 - Loss:  1.116, Seconds: 11.77\n",
            "Epoch  23/150 Batch  560/781 - Loss:  1.171, Seconds: 10.81\n",
            "Epoch  23/150 Batch  580/781 - Loss:  1.228, Seconds: 11.12\n",
            "Epoch  23/150 Batch  600/781 - Loss:  1.167, Seconds: 11.35\n",
            "Epoch  23/150 Batch  620/781 - Loss:  1.209, Seconds: 11.85\n",
            "Epoch  23/150 Batch  640/781 - Loss:  1.176, Seconds: 11.71\n",
            "Epoch  23/150 Batch  660/781 - Loss:  1.172, Seconds: 11.73\n",
            "Epoch  23/150 Batch  680/781 - Loss:  1.123, Seconds: 10.63\n",
            "Epoch  23/150 Batch  700/781 - Loss:  1.241, Seconds: 11.28\n",
            "Epoch  23/150 Batch  720/781 - Loss:  1.185, Seconds: 11.96\n",
            "Epoch  23/150 Batch  740/781 - Loss:  1.128, Seconds: 11.85\n",
            "Epoch  23/150 Batch  760/781 - Loss:  1.228, Seconds: 11.13\n",
            "Average loss for this update: 1.179\n",
            "No Improvement.\n",
            "Epoch  23/150 Batch  780/781 - Loss:  1.170, Seconds: 11.31\n",
            "Epoch  24/150 Batch   20/781 - Loss:  1.209, Seconds: 10.25\n",
            "Epoch  24/150 Batch   40/781 - Loss:  1.100, Seconds: 9.89\n",
            "Epoch  24/150 Batch   60/781 - Loss:  1.038, Seconds: 10.14\n",
            "Epoch  24/150 Batch   80/781 - Loss:  1.135, Seconds: 9.69\n",
            "Epoch  24/150 Batch  100/781 - Loss:  1.145, Seconds: 10.49\n",
            "Epoch  24/150 Batch  120/781 - Loss:  1.089, Seconds: 10.98\n",
            "Epoch  24/150 Batch  140/781 - Loss:  1.133, Seconds: 10.28\n",
            "Epoch  24/150 Batch  160/781 - Loss:  1.733, Seconds: 10.29\n",
            "Epoch  24/150 Batch  180/781 - Loss:  1.141, Seconds: 10.28\n",
            "Epoch  24/150 Batch  200/781 - Loss:  1.040, Seconds: 10.94\n",
            "Epoch  24/150 Batch  220/781 - Loss:  1.281, Seconds: 11.07\n",
            "Epoch  24/150 Batch  240/781 - Loss:  1.272, Seconds: 10.26\n",
            "Average loss for this update: 1.194\n",
            "No Improvement.\n",
            "Epoch  24/150 Batch  260/781 - Loss:  1.212, Seconds: 10.97\n",
            "Epoch  24/150 Batch  280/781 - Loss:  1.204, Seconds: 11.08\n",
            "Epoch  24/150 Batch  300/781 - Loss:  1.273, Seconds: 12.88\n",
            "Epoch  24/150 Batch  320/781 - Loss:  1.180, Seconds: 10.81\n",
            "Epoch  24/150 Batch  340/781 - Loss:  1.095, Seconds: 10.31\n",
            "Epoch  24/150 Batch  360/781 - Loss:  1.175, Seconds: 10.25\n",
            "Epoch  24/150 Batch  380/781 - Loss:  1.201, Seconds: 10.99\n",
            "Epoch  24/150 Batch  400/781 - Loss:  1.141, Seconds: 10.98\n",
            "Epoch  24/150 Batch  420/781 - Loss:  1.109, Seconds: 11.36\n",
            "Epoch  24/150 Batch  440/781 - Loss:  1.177, Seconds: 10.84\n",
            "Epoch  24/150 Batch  460/781 - Loss:  1.150, Seconds: 11.64\n",
            "Epoch  24/150 Batch  480/781 - Loss:  1.102, Seconds: 11.05\n",
            "Epoch  24/150 Batch  500/781 - Loss:  1.133, Seconds: 11.16\n",
            "Average loss for this update: 1.157\n",
            "No Improvement.\n",
            "Epoch  24/150 Batch  520/781 - Loss:  1.088, Seconds: 11.18\n",
            "Epoch  24/150 Batch  540/781 - Loss:  1.059, Seconds: 11.55\n",
            "Epoch  24/150 Batch  560/781 - Loss:  1.106, Seconds: 10.98\n",
            "Epoch  24/150 Batch  580/781 - Loss:  1.151, Seconds: 11.06\n",
            "Epoch  24/150 Batch  600/781 - Loss:  1.096, Seconds: 11.24\n",
            "Epoch  24/150 Batch  620/781 - Loss:  1.144, Seconds: 11.60\n",
            "Epoch  24/150 Batch  640/781 - Loss:  1.103, Seconds: 11.68\n",
            "Epoch  24/150 Batch  660/781 - Loss:  1.099, Seconds: 11.58\n",
            "Epoch  24/150 Batch  680/781 - Loss:  1.063, Seconds: 10.70\n",
            "Epoch  24/150 Batch  700/781 - Loss:  1.174, Seconds: 11.62\n",
            "Epoch  24/150 Batch  720/781 - Loss:  1.124, Seconds: 12.06\n",
            "Epoch  24/150 Batch  740/781 - Loss:  1.071, Seconds: 11.66\n",
            "Epoch  24/150 Batch  760/781 - Loss:  1.177, Seconds: 11.14\n",
            "Average loss for this update: 1.115\n",
            "No Improvement.\n",
            "Epoch  24/150 Batch  780/781 - Loss:  1.122, Seconds: 11.49\n",
            "Epoch  25/150 Batch   20/781 - Loss:  1.142, Seconds: 10.30\n",
            "Epoch  25/150 Batch   40/781 - Loss:  1.043, Seconds: 10.17\n",
            "Epoch  25/150 Batch   60/781 - Loss:  0.989, Seconds: 10.58\n",
            "Epoch  25/150 Batch   80/781 - Loss:  1.076, Seconds: 9.89\n",
            "Epoch  25/150 Batch  100/781 - Loss:  1.079, Seconds: 10.09\n",
            "Epoch  25/150 Batch  120/781 - Loss:  1.024, Seconds: 10.66\n",
            "Epoch  25/150 Batch  140/781 - Loss:  1.087, Seconds: 10.37\n",
            "Epoch  25/150 Batch  160/781 - Loss:  1.260, Seconds: 10.38\n",
            "Epoch  25/150 Batch  180/781 - Loss:  1.084, Seconds: 10.23\n",
            "Epoch  25/150 Batch  200/781 - Loss:  0.996, Seconds: 10.35\n",
            "Epoch  25/150 Batch  220/781 - Loss:  1.250, Seconds: 11.07\n",
            "Epoch  25/150 Batch  240/781 - Loss:  1.440, Seconds: 10.40\n",
            "Average loss for this update: 1.168\n",
            "No Improvement.\n",
            "Epoch  25/150 Batch  260/781 - Loss:  1.740, Seconds: 10.76\n",
            "Epoch  25/150 Batch  280/781 - Loss:  1.561, Seconds: 11.16\n",
            "Epoch  25/150 Batch  300/781 - Loss:  1.591, Seconds: 10.63\n",
            "Epoch  25/150 Batch  320/781 - Loss:  1.443, Seconds: 10.59\n",
            "Epoch  25/150 Batch  340/781 - Loss:  1.332, Seconds: 10.49\n",
            "Epoch  25/150 Batch  360/781 - Loss:  1.420, Seconds: 10.55\n",
            "Epoch  25/150 Batch  380/781 - Loss:  1.414, Seconds: 10.79\n",
            "Epoch  25/150 Batch  400/781 - Loss:  1.326, Seconds: 10.70\n",
            "Epoch  25/150 Batch  420/781 - Loss:  1.300, Seconds: 11.06\n",
            "Epoch  25/150 Batch  440/781 - Loss:  1.387, Seconds: 11.04\n",
            "Epoch  25/150 Batch  460/781 - Loss:  1.305, Seconds: 11.41\n",
            "Epoch  25/150 Batch  480/781 - Loss:  1.252, Seconds: 10.78\n",
            "Epoch  25/150 Batch  500/781 - Loss:  1.320, Seconds: 11.22\n",
            "Average loss for this update: 1.38\n",
            "No Improvement.\n",
            "Epoch  25/150 Batch  520/781 - Loss:  1.270, Seconds: 10.97\n",
            "Epoch  25/150 Batch  540/781 - Loss:  1.198, Seconds: 11.40\n",
            "Epoch  25/150 Batch  560/781 - Loss:  1.230, Seconds: 10.92\n",
            "Epoch  25/150 Batch  580/781 - Loss:  1.262, Seconds: 12.53\n",
            "Epoch  25/150 Batch  600/781 - Loss:  1.199, Seconds: 11.37\n",
            "Epoch  25/150 Batch  620/781 - Loss:  1.237, Seconds: 11.54\n",
            "Epoch  25/150 Batch  640/781 - Loss:  1.212, Seconds: 11.81\n",
            "Epoch  25/150 Batch  660/781 - Loss:  1.200, Seconds: 11.82\n",
            "Epoch  25/150 Batch  680/781 - Loss:  1.153, Seconds: 11.51\n",
            "Epoch  25/150 Batch  700/781 - Loss:  1.273, Seconds: 11.36\n",
            "Epoch  25/150 Batch  720/781 - Loss:  1.221, Seconds: 11.90\n",
            "Epoch  25/150 Batch  740/781 - Loss:  1.151, Seconds: 11.75\n",
            "Epoch  25/150 Batch  760/781 - Loss:  1.262, Seconds: 11.14\n",
            "Average loss for this update: 1.216\n",
            "No Improvement.\n",
            "Epoch  25/150 Batch  780/781 - Loss:  1.192, Seconds: 11.80\n",
            "Epoch  26/150 Batch   20/781 - Loss:  1.245, Seconds: 10.11\n",
            "Epoch  26/150 Batch   40/781 - Loss:  1.112, Seconds: 9.96\n",
            "Epoch  26/150 Batch   60/781 - Loss:  1.062, Seconds: 10.21\n",
            "Epoch  26/150 Batch   80/781 - Loss:  1.162, Seconds: 9.72\n",
            "Epoch  26/150 Batch  100/781 - Loss:  1.158, Seconds: 10.27\n",
            "Epoch  26/150 Batch  120/781 - Loss:  1.098, Seconds: 10.84\n",
            "Epoch  26/150 Batch  140/781 - Loss:  1.145, Seconds: 10.13\n",
            "Epoch  26/150 Batch  160/781 - Loss:  1.524, Seconds: 10.28\n",
            "Epoch  26/150 Batch  180/781 - Loss:  1.159, Seconds: 10.27\n",
            "Epoch  26/150 Batch  200/781 - Loss:  1.052, Seconds: 10.70\n",
            "Epoch  26/150 Batch  220/781 - Loss:  1.373, Seconds: 11.43\n",
            "Epoch  26/150 Batch  240/781 - Loss:  1.280, Seconds: 10.27\n",
            "Average loss for this update: 1.198\n",
            "No Improvement.\n",
            "Epoch  26/150 Batch  260/781 - Loss:  1.207, Seconds: 10.49\n",
            "Epoch  26/150 Batch  280/781 - Loss:  1.210, Seconds: 11.33\n",
            "Epoch  26/150 Batch  300/781 - Loss:  1.257, Seconds: 10.87\n",
            "Epoch  26/150 Batch  320/781 - Loss:  1.190, Seconds: 10.85\n",
            "Epoch  26/150 Batch  340/781 - Loss:  1.097, Seconds: 10.22\n",
            "Epoch  26/150 Batch  360/781 - Loss:  1.173, Seconds: 10.25\n",
            "Epoch  26/150 Batch  380/781 - Loss:  1.199, Seconds: 10.70\n",
            "Epoch  26/150 Batch  400/781 - Loss:  1.134, Seconds: 11.01\n",
            "Epoch  26/150 Batch  420/781 - Loss:  1.108, Seconds: 10.63\n",
            "Epoch  26/150 Batch  440/781 - Loss:  1.168, Seconds: 11.24\n",
            "Epoch  26/150 Batch  460/781 - Loss:  1.138, Seconds: 11.22\n",
            "Epoch  26/150 Batch  480/781 - Loss:  1.088, Seconds: 10.88\n",
            "Epoch  26/150 Batch  500/781 - Loss:  1.133, Seconds: 11.39\n",
            "Average loss for this update: 1.152\n",
            "No Improvement.\n",
            "Epoch  26/150 Batch  520/781 - Loss:  1.074, Seconds: 11.02\n",
            "Epoch  26/150 Batch  540/781 - Loss:  1.042, Seconds: 11.71\n",
            "Epoch  26/150 Batch  560/781 - Loss:  1.115, Seconds: 10.89\n",
            "Epoch  26/150 Batch  580/781 - Loss:  1.133, Seconds: 11.26\n",
            "Epoch  26/150 Batch  600/781 - Loss:  1.088, Seconds: 11.21\n",
            "Epoch  26/150 Batch  620/781 - Loss:  1.137, Seconds: 11.66\n",
            "Epoch  26/150 Batch  640/781 - Loss:  1.100, Seconds: 11.72\n",
            "Epoch  26/150 Batch  660/781 - Loss:  1.095, Seconds: 11.57\n",
            "Epoch  26/150 Batch  680/781 - Loss:  1.052, Seconds: 10.40\n",
            "Epoch  26/150 Batch  700/781 - Loss:  1.165, Seconds: 11.36\n",
            "Epoch  26/150 Batch  720/781 - Loss:  1.109, Seconds: 11.86\n",
            "Epoch  26/150 Batch  740/781 - Loss:  1.051, Seconds: 11.78\n",
            "Epoch  26/150 Batch  760/781 - Loss:  1.163, Seconds: 10.99\n",
            "Average loss for this update: 1.105\n",
            "No Improvement.\n",
            "Epoch  26/150 Batch  780/781 - Loss:  1.100, Seconds: 11.73\n",
            "Epoch  27/150 Batch   20/781 - Loss:  1.140, Seconds: 10.12\n",
            "Epoch  27/150 Batch   40/781 - Loss:  1.036, Seconds: 10.05\n",
            "Epoch  27/150 Batch   60/781 - Loss:  0.983, Seconds: 10.15\n",
            "Epoch  27/150 Batch   80/781 - Loss:  1.070, Seconds: 9.69\n",
            "Epoch  27/150 Batch  100/781 - Loss:  1.069, Seconds: 10.31\n",
            "Epoch  27/150 Batch  120/781 - Loss:  1.011, Seconds: 10.66\n",
            "Epoch  27/150 Batch  140/781 - Loss:  1.062, Seconds: 10.05\n",
            "Epoch  27/150 Batch  160/781 - Loss:  1.187, Seconds: 10.23\n",
            "Epoch  27/150 Batch  180/781 - Loss:  1.072, Seconds: 10.45\n",
            "Epoch  27/150 Batch  200/781 - Loss:  0.977, Seconds: 10.68\n",
            "Epoch  27/150 Batch  220/781 - Loss:  1.457, Seconds: 11.14\n",
            "Epoch  27/150 Batch  240/781 - Loss:  1.216, Seconds: 10.17\n",
            "Average loss for this update: 1.11\n",
            "No Improvement.\n",
            "Epoch  27/150 Batch  260/781 - Loss:  1.146, Seconds: 10.51\n",
            "Epoch  27/150 Batch  280/781 - Loss:  1.154, Seconds: 10.99\n",
            "Epoch  27/150 Batch  300/781 - Loss:  1.194, Seconds: 10.80\n",
            "Epoch  27/150 Batch  320/781 - Loss:  1.131, Seconds: 10.99\n",
            "Epoch  27/150 Batch  340/781 - Loss:  1.039, Seconds: 10.15\n",
            "Epoch  27/150 Batch  360/781 - Loss:  1.121, Seconds: 10.45\n",
            "Epoch  27/150 Batch  380/781 - Loss:  1.153, Seconds: 10.90\n",
            "Epoch  27/150 Batch  400/781 - Loss:  1.098, Seconds: 10.64\n",
            "Epoch  27/150 Batch  420/781 - Loss:  1.062, Seconds: 11.18\n",
            "Epoch  27/150 Batch  440/781 - Loss:  1.113, Seconds: 10.87\n",
            "Epoch  27/150 Batch  460/781 - Loss:  1.081, Seconds: 11.33\n",
            "Epoch  27/150 Batch  480/781 - Loss:  1.040, Seconds: 11.11\n",
            "Epoch  27/150 Batch  500/781 - Loss:  1.096, Seconds: 11.41\n",
            "Average loss for this update: 1.101\n",
            "No Improvement.\n",
            "Stopping Training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uWRgQNWcgYfq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def text_to_seq(text):\n",
        "    '''Prepare the text for the model'''\n",
        "    \n",
        "    text = clean_text(text)\n",
        "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XpLf5o7AgYfs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a3fbeb6d-89d9-4a9e-a2fb-33d2099fde3b"
      },
      "cell_type": "code",
      "source": [
        "# Create your own review or use one from the dataset\n",
        "#input_sentence = \"\"\n",
        "#text = text_to_seq(input_sentence)\n",
        "random = np.random.randint(0,len(clean_texts))\n",
        "input_sentence = \"echo of the sea is my love for you what guitar composed was my affection\"\n",
        "text = text_to_seq(input_sentence)\n",
        "# clean_texts[390]\n",
        "checkpoint = \"./output/model.ckpt\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "# Remove the padding from the tweet\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Text:', input_sentence)\n",
        "print('\\nSummary')\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./output/model.ckpt\n",
            "Original Text: echo of the sea is my love for you what guitar composed was my affection\n",
            "\n",
            "Summary\n",
            "  Response Words: great for cleaning teeth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-gnnYPPHgYfu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}